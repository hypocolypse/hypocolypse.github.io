<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>: No 3. Assembly &amp; Annotations</title>
  
  <meta property="description" itemprop="description" content="This section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions."/>
  
  <link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
  <link rel="icon" type="image/vnd.microsoft.icon" href="assets/favicon.ico"/>
  
  <meta name="article:author" content="Jarrod J Scott"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content=": No 3. Assembly &amp; Annotations"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="This section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions."/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:site_name" content=""/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content=": No 3. Assembly &amp; Annotations"/>
  <meta property="twitter:description" content="This section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions."/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Anvi’o: An advanced analysis and visualization platform for ‘omics data;citation_publication_date=2015;citation_volume=3;citation_author=A Murat Eren;citation_author=Özcan C Esen;citation_author=Christopher Quince;citation_author=Joseph H Vineis;citation_author=Hilary G Morrison;citation_author=Mitchell L Sogin;citation_author=Tom O Delmont"/>
  <meta name="citation_reference" content="citation_title=Trimmomatic: A flexible trimmer for illumina sequence data;citation_publication_date=2014;citation_volume=30;citation_author=Anthony M Bolger;citation_author=Marc Lohse;citation_author=Bjoern Usadel"/>
  <meta name="citation_reference" content="citation_title=Snakemake—a scalable bioinformatics workflow engine;citation_publication_date=2012;citation_volume=28;citation_author=Johannes Köster;citation_author=Sven Rahmann"/>
  <meta name="citation_reference" content="citation_title=Fast and sensitive taxonomic classification for metagenomics with kaiju;citation_publication_date=2016;citation_volume=7;citation_author=Peter Menzel;citation_author=Kim Lee Ng;citation_author=Anders Krogh"/>
  <meta name="citation_reference" content="citation_title=VirSorter: Mining viral signal from microbial genomic data;citation_publication_date=2015;citation_volume=3;citation_author=Simon Roux;citation_author=Francois Enault;citation_author=Bonnie L Hurwitz;citation_author=Matthew B Sullivan"/>
  <meta name="citation_reference" content="citation_title=Centrifuge: Rapid and sensitive classification of metagenomic sequences;citation_publication_date=2016;citation_volume=26;citation_author=Daehwan Kim;citation_author=Li Song;citation_author=Florian P Breitwieser;citation_author=Steven L Salzberg"/>
  <meta name="citation_reference" content="citation_title=Prodigal: Prokaryotic gene recognition and translation initiation site identification;citation_publication_date=2010;citation_volume=11;citation_author=Doug Hyatt;citation_author=Gwo-Liang Chen;citation_author=Philip F LoCascio;citation_author=Miriam L Land;citation_author=Frank W Larimer;citation_author=Loren J Hauser"/>
  <meta name="citation_reference" content="citation_title=MEGAHIT: An ultra-fast single-node solution for large and complex metagenomics assembly via succinct de bruijn graph;citation_publication_date=2015;citation_volume=31;citation_author=Dinghua Li;citation_author=Chi-Man Liu;citation_author=Ruibang Luo;citation_author=Kunihiko Sadakane;citation_author=Tak-Wah Lam"/>
  <meta name="citation_reference" content="citation_title=Fast gapped-read alignment with bowtie 2;citation_publication_date=2012;citation_volume=9;citation_author=Ben Langmead;citation_author=Steven L Salzberg"/>
  <meta name="citation_reference" content="citation_title=The sequence alignment/map format and samtools;citation_publication_date=2009;citation_volume=25;citation_author=Heng Li;citation_author=Bob Handsaker;citation_author=Alec Wysoker;citation_author=Tim Fennell;citation_author=Jue Ruan;citation_author=Nils Homer;citation_author=Gabor Marth;citation_author=Goncalo Abecasis;citation_author=Richard Durbin"/>
  <meta name="citation_reference" content="citation_title=KrakenUniq: Confident and fast metagenomics classification using unique k-mer counts;citation_publication_date=2018;citation_volume=19;citation_author=FP Breitwieser;citation_author=DN Baker;citation_author=Steven L Salzberg"/>
  <meta name="citation_reference" content="citation_title=A filtering method to generate high quality short reads using illumina paired-end technology;citation_publication_date=2013;citation_volume=8;citation_author=A Murat Eren;citation_author=Joseph H Vineis;citation_author=Hilary G Morrison;citation_author=Mitchell L Sogin"/>
  <meta name="citation_reference" content="citation_title=Interactive metagenomic visualization in a web browser;citation_publication_date=2011;citation_volume=12;citation_author=Brian D Ondov;citation_author=Nicholas H Bergman;citation_author=Adam M Phillippy"/>
  <meta name="citation_reference" content="citation_title=Fast and sensitive protein alignment using diamond;citation_publication_date=2015;citation_volume=12;citation_author=Benjamin Buchfink;citation_author=Chao Xie;citation_author=Daniel H Huson"/>
  <meta name="citation_reference" content="citation_title=Nitrogen-fixing populations of planctomycetes and proteobacteria are abundant in surface ocean metagenomes;citation_publication_date=2018;citation_volume=3;citation_author=Tom O Delmont;citation_author=Christopher Quince;citation_author=Alon Shaiber;citation_author=Özcan C Esen;citation_author=Sonny TM Lee;citation_author=Michael S Rappé;citation_author=Sandra L McLellan;citation_author=Sebastian Lücker;citation_author=A Murat Eren"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","bibliography"]}},"value":[{"type":"character","attributes":{},"value":["No 3. Assembly & Annotations"]},{"type":"character","attributes":{},"value":["This section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name"]}},"value":[{"type":"character","attributes":{},"value":["Jarrod J Scott"]}]}]},{"type":"character","attributes":{},"value":["assets/cite.bib"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  <!--radix_placeholder_navigation_in_header-->
  
  <script type="application/javascript">
  
    window.headroom_prevent_pin = false;
  
    window.document.addEventListener("DOMContentLoaded", function (event) {
  
      // initialize headroom for banner
      var header = $('header').get(0);
      var headerHeight = header.offsetHeight;
      var headroom = new Headroom(header, {
        onPin : function() {
          if (window.headroom_prevent_pin) {
            window.headroom_prevent_pin = false;
            headroom.unpin();
          }
        }
      });
      headroom.init();
      if(window.location.hash)
        headroom.unpin();
      $(header).addClass('headroom--transition');
  
      // offset scroll location for banner on hash change
      // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
      window.addEventListener("hashchange", function(event) {
        window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
      });
  
      // responsive menu
      $('.distill-site-header').each(function(i, val) {
        var topnav = $(this);
        var toggle = topnav.find('.nav-toggle');
        toggle.on('click', function() {
          topnav.toggleClass('responsive');
        });
      });
  
      // nav dropdowns
      $('.nav-dropbtn').click(function(e) {
        $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
        $(this).parent().siblings('.nav-dropdown')
           .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
      });
      $("body").click(function(e){
        $('.nav-dropdown-content').removeClass('nav-dropdown-active');
      });
      $(".nav-dropdown").click(function(e){
        e.stopPropagation();
      });
    });
  </script>
  
  <style type="text/css">
  
  /* Theme (user-documented overrideables for nav appearance) */
  
  .distill-site-nav {
    color: rgba(255, 255, 255, 0.8);
    background-color: #455a64;
    font-size: 15px;
    font-weight: 300;
  }
  
  .distill-site-nav a {
    color: inherit;
    text-decoration: none;
  }
  
  .distill-site-nav a:hover {
    color: white;
  }
  
  @media print {
    .distill-site-nav {
      display: none;
    }
  }
  
  .distill-site-header {
  
  }
  
  .distill-site-footer {
  
  }
  
  
  /* Site Header */
  
  .distill-site-header {
    width: 100%;
    box-sizing: border-box;
    z-index: 3;
  }
  
  .distill-site-header .nav-left {
    display: inline-block;
    margin-left: 8px;
  }
  
  @media screen and (max-width: 768px) {
    .distill-site-header .nav-left {
      margin-left: 0;
    }
  }
  
  
  .distill-site-header .nav-right {
    float: right;
    margin-right: 8px;
  }
  
  .distill-site-header a,
  .distill-site-header .title {
    display: inline-block;
    text-align: center;
    padding: 14px 10px 14px 10px;
  }
  
  .distill-site-header .title {
    font-size: 18px;
  }
  
  .distill-site-header .logo {
    padding: 0;
  }
  
  .distill-site-header .logo img {
    display: none;
    max-height: 20px;
    width: auto;
    margin-bottom: -4px;
  }
  
  .distill-site-header .nav-image img {
    max-height: 18px;
    width: auto;
    display: inline-block;
    margin-bottom: -3px;
  }
  
  
  
  @media screen and (min-width: 1000px) {
    .distill-site-header .logo img {
      display: inline-block;
    }
    .distill-site-header .nav-left {
      margin-left: 20px;
    }
    .distill-site-header .nav-right {
      margin-right: 20px;
    }
    .distill-site-header .title {
      padding-left: 12px;
    }
  }
  
  
  .distill-site-header .nav-toggle {
    display: none;
  }
  
  .nav-dropdown {
    display: inline-block;
    position: relative;
  }
  
  .nav-dropdown .nav-dropbtn {
    border: none;
    outline: none;
    color: rgba(255, 255, 255, 0.8);
    padding: 16px 10px;
    background-color: transparent;
    font-family: inherit;
    font-size: inherit;
    font-weight: inherit;
    margin: 0;
    margin-top: 1px;
    z-index: 2;
  }
  
  .nav-dropdown-content {
    display: none;
    position: absolute;
    background-color: white;
    min-width: 200px;
    border: 1px solid rgba(0,0,0,0.15);
    border-radius: 4px;
    box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
    z-index: 1;
    margin-top: 2px;
    white-space: nowrap;
    padding-top: 4px;
    padding-bottom: 4px;
  }
  
  .nav-dropdown-content hr {
    margin-top: 4px;
    margin-bottom: 4px;
    border: none;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .nav-dropdown-active {
    display: block;
  }
  
  .nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
    color: black;
    padding: 6px 24px;
    text-decoration: none;
    display: block;
    text-align: left;
  }
  
  .nav-dropdown-content .nav-dropdown-header {
    display: block;
    padding: 5px 24px;
    padding-bottom: 0;
    text-transform: uppercase;
    font-size: 14px;
    color: #999999;
    white-space: nowrap;
  }
  
  .nav-dropdown:hover .nav-dropbtn {
    color: white;
  }
  
  .nav-dropdown-content a:hover {
    background-color: #ddd;
    color: black;
  }
  
  .nav-right .nav-dropdown-content {
    margin-left: -45%;
    right: 0;
  }
  
  @media screen and (max-width: 768px) {
    .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
    .distill-site-header a.nav-toggle {
      float: right;
      display: block;
    }
    .distill-site-header .title {
      margin-left: 0;
    }
    .distill-site-header .nav-right {
      margin-right: 0;
    }
    .distill-site-header {
      overflow: hidden;
    }
    .nav-right .nav-dropdown-content {
      margin-left: 0;
    }
  }
  
  
  @media screen and (max-width: 768px) {
    .distill-site-header.responsive {position: relative;}
    .distill-site-header.responsive a.nav-toggle {
      position: absolute;
      right: 0;
      top: 0;
    }
    .distill-site-header.responsive a,
    .distill-site-header.responsive .nav-dropdown {
      display: block;
      text-align: left;
    }
    .distill-site-header.responsive .nav-left,
    .distill-site-header.responsive .nav-right {
      width: 100%;
    }
    .distill-site-header.responsive .nav-dropdown {float: none;}
    .distill-site-header.responsive .nav-dropdown-content {position: relative;}
    .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
      display: block;
      width: 100%;
      text-align: left;
    }
  }
  
  /* Site Footer */
  
  .distill-site-footer {
    width: 100%;
    overflow: hidden;
    box-sizing: border-box;
    z-index: 3;
    margin-top: 30px;
    padding-top: 30px;
    padding-bottom: 30px;
    text-align: center;
  }
  
  /* Headroom */
  
  d-title {
    padding-top: 6rem;
  }
  
  @media print {
    d-title {
      padding-top: 4rem;
    }
  }
  
  .headroom {
    z-index: 1000;
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
  }
  
  .headroom--transition {
    transition: all .4s ease-in-out;
  }
  
  .headroom--unpinned {
    top: -100px;
  }
  
  .headroom--pinned {
    top: 0;
  }
  
  </style>
  
  <link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
  <link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
  <script src="site_libs/headroom-0.9.4/headroom.min.js"></script>
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <style type="text/css">
  /* Whole document: */
  @import url('https://fonts.googleapis.com/css?family=PT+Sans|PT+Serif|Lato:300|Roboto:300|Nunito+Sans:300|Inconsolata|Source+Sans+Pro');
  /* controls general properties of site*/
  body {
    color: #414141;
  /*  line-height: 1.7;
    font-size: 17px; */
    background-color: #FAFAFA;
  }
  
  d-title h1{
  font-size: 2.5em;
  font-family: Roboto;
  font-weight: 500;
  }
  
  d-article h1, h2 {
    font-family: Roboto;
  }
  
  d-article h3 {
    margin-top: 1em;
  }
  
  blockquote p {
  	font-size: 1.3em;
  	font-family: 'Lato';
    font-style: normal;
    margin: 0.5em 0 0.5em;
  }
  
  d-code {
    background-color: #E8E8E8;
    border-radius: 3px;
    font-size: 0.8em;
    border-left: 5px solid #FC6464;
    font-family: 'Monaco', 'PT Mono', monospace;
    overflow-x: scroll !important;
    margin-bottom: 20px;
  }
  
  pre.text-output {
    margin-top: 0px;
     color: #333333;
     background-color: #FAFAFA;
     font-size: 0.8em
   }
  
  pre {
    border: 1px solid #ccc;
    padding: 0px 10px 10px 18px;
    margin: 1em 0px;
    border-radius: 5px;
  }
  
  .nav-dropdown-content .nav-dropdown-header {
    text-transform: none;
  }
  
  d-article li {
    margin: 0px 0px 4px;
  }
  
  /*************************************************
  *  code for side by side krona plot
  **************************************************/
  
  .column {
    float: left;
    width: 48%;
    padding: 5px;
  }
  
  /* Clear floats after image containers */
  .row::after {
    content: "";
    clear: both;
    display: table;
  }
  
  div.krona {
    width: 100%;
  	height: 90%;
    background-color: white;
    box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
    margin-bottom: 25px;
  	margin-top: 5px;
  }
  
  div.container-krona {
    text-align: center;
    padding: 1px 10px;
  	color: #414141;
  	font-size: 1.48em;
  	line-height: 0.8em;
  	font-family: 'Inconsolata';
  }
  
  div.container-krona p {
    margin: 20px;
  }
  
  div.krona img {
    border:1px solid white;
    margin: 25px;
  }
  
  img {
    margin: 25px;
  }
  
  blockquote p {
  	font-size: 1.3em;
  	font-family: 'Lato';
  }
  
  /** LIGHTBOX MARKUP **/
  .thumbnail {
    max-width: 100%;
    position: relative;
  }
  
  .lightbox {
  	/** Default lightbox to hidden */
  	display: none;
  
  	/** Position and style */
  	position: fixed;
  	z-index: 999;
  	width: 100%;
  	height: 100%;
  	text-align: center;
  	top: 0;
  	left: 0;
  	background: rgba(0,0,0,0.8);
  }
  
  .lightbox img {
  	/** Pad the lightbox image */
  	position: fixed;
  	top: 0;
  	left: 0;
  	right: 0;
  	bottom: 0;
  	max-width: 0%;
  	max-height: 0%;
  	margin: auto;
  	box-sizing: border-box;
  }
  
  .lightbox:target {
  	/** Remove default browser outline */
  	outline: none;
  
  	/** Unhide lightbox **/
  	display: block;
  }
  
  .lightbox:target img {
  	max-height: 100%;
  	max-width: 100%;
  }
  
  .distill-site-header .logo img {
    margin-top: 10px;
    max-height: 30px;
    margin-bottom: 10px;
  }
  
  .distill-site-nav {
      color: rgba(255, 255, 255, 0.8);
      background-color: #143D59;
      font-size: 16px;
      font-weight: 300;
    }
  
  </style>
  <!--/radix_placeholder_site_in_header-->

  <link rel="stylesheet" href="assets/styles.css" type="text/css"/>

</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"No 3. Assembly & Annotations","description":"This section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions.","authors":[{"author":"Jarrod J Scott","authorURL":"#","affiliation":"&nbsp;","affiliationURL":"#"}]}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a class="logo" href="index.html">
<img src="assets/icon.png"/>
</a>
<a href="index.html" class="title"></a>
</div>
<div class="nav-right">
<a href="index.html">
<i class="fa fa-home"></i>
</a>
<a href="intro.html">Introduction</a>
<a href="field.html">A. Field Analyses</a>
<div class="nav-dropdown">
<button class="nav-dropbtn">
B. 16S rRNA
 
<span class="down-arrow">&#x25BE;</span>
</button>
<div class="nav-dropdown-content">
<span class="nav-dropdown-header">16S rRNA Processing</span>
<a href="16s-dada2.html">1. DADA2</a>
<a href="16s-data-prep.html">2. Data Prep</a>
<hr/>
<span class="nav-dropdown-header">16S rRNA Analyses</span>
<a href="16s-water.html">3. Water Diversity</a>
<hr/>
<a href="session-info.html">Session Info</a>
</div>
</div>
<div class="nav-dropdown">
<button class="nav-dropbtn">
C. Metagenomic
 
<span class="down-arrow">&#x25BE;</span>
</button>
<div class="nav-dropdown-content">
<span class="nav-dropdown-header">Setup</span>
<a href="mg-setup.html">1. Working Envrionment</a>
<a href="mg-databases.html">2. Annotation Databases</a>
<hr/>
<span class="nav-dropdown-header">Read Processing</span>
<a href="mg-workflow-1.html">3. Assembly &amp; Annotations</a>
<a href="mg-workflow-2.html">4. Assembley &amp; Annotation Summary</a>
<hr/>
<span class="nav-dropdown-header">Binning</span>
<a href="mg-auto-binning.html">5. Automated Binning</a>
</div>
</div>
<a href="synthesis.html">D. Synthesis</a>
<a href="data-availability.html">Data</a>
<a href="https://github.com/hypocolypse/web/">
<i class="fa fa-github fa-lg"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>No 3. Assembly &amp; Annotations</h1>
<p>This section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions.</p>
</div>

<div class="d-byline">
  Jarrod J Scott  
  

</div>

<div class="d-article">
<h3 class="d-toc-header">Table of Contents</h3>
<nav class="d-toc" id="TOC">
<ul>
<li><a href="#the-workflow">The Workflow</a></li>
<li><a href="#trimming">Trimming</a></li>
<li><a href="#snakemake-workflow">Snakemake Workflow</a></li>
<li><a href="#taxonomic-annotations">Taxonomic Annotations</a><ul>
<li><a href="#kaken-annotation">Kaken Annotation</a></li>
<li><a href="#virsorter-annotation">VirSorter Annotation</a></li>
<li><a href="#kaiju-annotation">Kaiju Annotation</a></li>
</ul></li>
<li><a href="#functional-annotations">Functional Annotations</a><ul>
<li><a href="#scg-taxonomy-pfam">SCG Taxonomy &amp; Pfam</a></li>
<li><a href="#ghostkoala-annotations">GhostKOALA Annotations</a></li>
</ul></li>
<li><a href="#merge-profiles-import-virsorter">Merge Profiles &amp; Import VirSorter</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#source-code">Source Code</a></li>
</ul>
</nav>
<hr class="d-toc-separator"/>
<h2 id="the-workflow">The Workflow</h2>
<p>In this section of the workflow we begin with raw, paired-end Illumina data. We will use a Snakemake<span class="citation" data-cites="koster2012snakemake">(Köster and Rahmann <a href="#ref-koster2012snakemake">2012</a>)</span> workflow in the anvi’o<span class="citation" data-cites="eren2015anvi">(Eren et al. <a href="#ref-eren2015anvi">2015</a>)</span> environment for most of the steps, though we will at time call on additonal tools for specific steps. The overall structure of our workflow was modelled after the one described by Delmont &amp; Eren on <a href="http://merenlab.org/data/tara-oceans-mags/">Recovering Microbial Genomes from TARA Oceans Metagenomes</a><span class="citation" data-cites="delmont2018nitrogen">(Delmont et al. <a href="#ref-delmont2018nitrogen">2018</a>)</span>.</p>
<p>Specifically we will:</p>
<ol type="1">
<li>Perform adapter trimming.</li>
<li>Use the anvi’o Snakemake workflow to:
<ol type="a">
<li>quality filter trimmed, raw reads;</li>
<li>co-assemble metagenomic sets;</li>
<li>recruit reads;</li>
<li>profile the mapping results;</li>
<li>merge profile dbs;</li>
<li>classify reads and genes;</li>
<li>annotate genes;</li>
<li>run HMM profiles.</li>
</ol></li>
<li>Use external tools for annotation and classification.</li>
</ol>
<p>The main steps of this workflow are:</p>
<div class="l-body-outset">
<table>
<colgroup>
<col style="width: 29%" />
<col style="width: 47%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Tool/Description</th>
<th>Input</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>TRIMMING</td>
<td>TRIMMOMATIC to remove adaptor sequences.</td>
<td>raw reads</td>
</tr>
<tr class="even">
<td>QUALITY-FILTERING</td>
<td>IU filter quality Minoche to QC trimmed reads.</td>
<td>trimmed reads</td>
</tr>
<tr class="odd">
<td>CO-ASSEMBLY</td>
<td>MEGAHIT to co-assemble metagenomic samples.</td>
<td>QC reads</td>
</tr>
<tr class="even">
<td>GENE CALLING</td>
<td>PRODIGAL for gene calling. Contig db for results.</td>
<td>assembled contigs</td>
</tr>
<tr class="odd">
<td>TAXONOMIC ANNOTATION</td>
<td>KrakenUniq for short read classification.</td>
<td>QC reads</td>
</tr>
<tr class="even">
<td>RECRUITMENT</td>
<td>BOWTIE2/SAMtools for mapping short reads to assemly.</td>
<td>QC reads, contigs db</td>
</tr>
<tr class="odd">
<td>PROFILING</td>
<td>Profile mapping results. Store in profile dbs.</td>
<td>BAM file</td>
</tr>
<tr class="even">
<td>TAXONOMIC ANNOTATION</td>
<td>KAIJU &amp; CENTRIFUGE for classification of genes.</td>
<td>genes (dna format)</td>
</tr>
<tr class="odd">
<td>FUNCTIONAL ANNOTATION</td>
<td>Gene annotations against Pfams, COGS, KEGG, etc.</td>
<td>genes (aa format)</td>
</tr>
<tr class="even">
<td>MERGING</td>
<td>Create single merged profile db.</td>
<td>individual profile dbs</td>
</tr>
</tbody>
</table>
</div>
<h2 id="trimming">Trimming</h2>
<p>Before we run the workflow we must trim Illumina adaptors. First we need to make some directories to put the output files.</p>
<pre class="bash"><code>
mkdir 00_TRIMMED
mkdir 00_TRIMMED_UNPAIRED</code></pre>
<p>Next we run TRIMMOMATIC<span class="citation" data-cites="bolger2014trimmomatic">(Bolger, Lohse, and Usadel <a href="#ref-bolger2014trimmomatic">2014</a>)</span> assuming the raw data is in a directory called <code>RAW</code>. Here is an example for a single sample (<code>WCCR_1913</code>). Since we ran these samples on a NextSeq, there are 4 files per paired-end. So we have 8 files total. We need to run the command 4 times, one for each pair. Here is the command for the first pair.</p>
<aside>
<a href="https://doi.org/10.1093/bioinformatics/btu170">GitHub Repo for TRIMMOMATIC</a>
</aside>
<pre class="bash"><code>
runtrimmomatic PE -threads $NSLOTS RAW/WCCR_1913_S74_L001_R1_001.fastq.gz /
                                   RAW/WCCR_1913_S74_L001_R2_001.fastq.gz /
                                   00_TRIMMED/WCCR_1913_S74_L001_R1_001_trim.fastq.gz /
                                   00_TRIMMED_UNPAIRED/WCCR_1913_S74_L001_R1_001_trim_unpaired.fastq.gz /
                                   00_TRIMMED/WCCR_1913_S74_L001_R2_001_trim.fastq.gz /
                                   00_TRIMMED_UNPAIRED/WCCR_1913_S74_L001_R2_001_trim_unpaired.fastq.gz /
          ILLUMINACLIP:/share/apps/bioinformatics/trimmomatic/0.33/adapters/NexteraPE-PE.fa:2:30:10 /
          MINLEN:40</code></pre>
<p>Now we can run the command for the other 3 pairs,</p>
<ul>
<li>WCCR_1913_S74_L002_R1_001 &amp; WCCR_1913_S74_L002_R2_001</li>
<li>WCCR_1913_S74_L003_R1_001 &amp; WCCR_1913_S74_L003_R2_001</li>
<li>WCCR_1913_S74_L004_R1_001 &amp; WCCR_1913_S74_L004_R2_001</li>
</ul>
<p>And then do the same for the rest of the samples.</p>
<details>
<summary>Show/hide HYDRA TRIMMOMATIC job script</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 16
#$ -q sThC.q
#$ -cwd
#$ -j y
#$ -N job_00_trimmomatic
#$ -o hydra_logs/job_00_trimmomatic_redo.log
#
# ----------------Modules------------------------- #
module load bioinformatics/trimmomatic
#
# ----------------Your Commands------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
# ----------------COMMANDS------------------- #
#
echo = `date` job $JOB_NAME don
</code></pre>
</details>
<h2 id="snakemake-workflow">Snakemake Workflow</h2>
<p>Thanks to the anvi’o implementation of Snakemake we can run many commands suquentially and/or simutaneously. There is plenty of good documentation on the anvi’o website about setting up snakemake workflows <a href="http://merenlab.org/2018/07/09/anvio-snakemake-workflows/">here</a> so we will refrain from any lengthy explainations.</p>
<p>You can see all the settings we used and download our configs file <a href="files/default-config.txt">here</a> or grab it from the folded code below. Some commands we chose not to run but they remain in the workflow for posterity.</p>
<details>
<summary>Show/hide JSON-formatted configuration file.</summary>
<pre><code>
{
    "fasta_txt": "",
    "workflow_name": "metagenomics",
    "anvi_gen_contigs_database": {
        "--project-name": "{group}",
        "--description": "",
        "--skip-gene-calling": "",
        "--external-gene-calls": "",
        "--ignore-internal-stop-codons": "",
        "--skip-mindful-splitting": "",
        "--contigs-fasta": "",
        "--split-length": "",
        "--kmer-size": "",
        "--prodigal-translation-table": "",
        "threads": ""
    },
    "centrifuge": {
        "threads": 12,
        "run": true,
        "db": "/pool/genomics/stri_istmobiome/dbs/centrifuge_dbs/p+h+v"
    },
    "anvi_run_hmms": {
        "run": true,
        "threads": 5,
        "--installed-hmm-profile": "",
        "--hmm-profile-dir": ""
    },
    "anvi_run_ncbi_cogs": {
        "run": true,
        "threads": 5,
        "--cog-data-dir": "/pool/genomics/stri_istmobiome/dbs/cog_db/",
        "--sensitive": "",
        "--temporary-dir-path": "/pool/genomics/stri_istmobiome/dbs/cog_db/tmp/",
        "--search-with": ""
    },
    "anvi_run_scg_taxonomy": {
        "run": true,
        "threads": 6,
        "--scgs-taxonomy-data-dir": "/pool/genomics/stri_istmobiome/dbs/scgs-taxonomy-data/"
    },
    "anvi_script_reformat_fasta": {
        "run": true,
        "--prefix": "{group}",
        "--simplify-names": true,
        "--keep-ids": "",
        "--exclude-ids": "",
        "--min-len": "1000",
        "threads": ""
    },
    "emapper": {
        "--database": "bact",
        "--usemem": true,
        "--override": true,
        "path_to_emapper_dir": "",
        "threads": ""
    },
    "anvi_script_run_eggnog_mapper": {
        "--use-version": "0.12.6",
        "run": "",
        "--cog-data-dir": "/pool/genomics/stri_istmobiome/dbs/cog_db/",
        "--drop-previous-annotations": "",
        "threads": 20
    },
    "samples_txt": "samples.txt",
    "metaspades": {
        "additional_params": "--only-assembler",
        "threads": 7,
        "run": "",
        "use_scaffolds": ""
    },
    "megahit": {
        "--min-contig-len": 1000,
        "--memory": 0.8,
        "threads": 20,
        "run": true,
        "--min-count": "",
        "--k-min": "",
        "--k-max": "",
        "--k-step": "",
        "--k-list": "",
        "--no-mercy": "",
        "--no-bubble": "",
        "--merge-level": "",
        "--prune-level": "",
        "--prune-depth": "",
        "--low-local-ratio": "",
        "--max-tip-len": "",
        "--no-local": "",
        "--kmin-1pass": "",
        "--presets": "meta-sensitive",
        "--mem-flag": "",
        "--use-gpu": "",
        "--gpu-mem": "",
        "--keep-tmp-files": "",
        "--tmp-dir": "",
        "--continue": true,
        "--verbose": ""
    },
    "idba_ud": {
        "--min_contig": 1000,
        "threads": 7,
        "run": "",
        "--mink": "",
        "--maxk": "",
        "--step": "",
        "--inner_mink": "",
        "--inner_step": "",
        "--prefix": "",
        "--min_count": "",
        "--min_support": "",
        "--seed_kmer": "",
        "--similar": "",
        "--max_mismatch": "",
        "--min_pairs": "",
        "--no_bubble": "",
        "--no_local": "",
        "--no_coverage": "",
        "--no_correct": "",
        "--pre_correction": ""
    },
    "iu_filter_quality_minoche": {
        "run": true,
        "--ignore-deflines": true,
        "--visualize-quality-curves": "",
        "--limit-num-pairs": "",
        "--print-qual-scores": "",
        "--store-read-fate": "",
        "threads": ""
    },
    "gzip_fastqs": {
        "run": true,
        "threads": ""
    },
    "bowtie": {
        "additional_params": "--no-unal",
        "threads": 3
    },
    "samtools_view": {
        "additional_params": "-F 4",
        "threads": ""
    },
    "anvi_profile": {
        "threads": 5,
        "--sample-name": "{sample}",
        "--overwrite-output-destinations": true,
        "--report-variability-full": "",
        "--skip-SNV-profiling": "",
        "--profile-SCVs": truw,
        "--description": "",
        "--skip-hierarchical-clustering": "",
        "--distance": "",
        "--linkage": "",
        "--min-contig-length": "",
        "--min-mean-coverage": "",
        "--min-coverage-for-variability": "",
        "--cluster-contigs": "",
        "--contigs-of-interest": "",
        "--queue-size": "",
        "--write-buffer-size": 10000,
        "--max-contig-length": "",
        "--max-coverage-depth": "",
        "--ignore-orphans": ""
    },
    "anvi_merge": {
        "--sample-name": "{group}",
        "--overwrite-output-destinations": true,
        "--description": "",
        "--skip-hierarchical-clustering": "",
        "--enforce-hierarchical-clustering": "",
        "--distance": "",
        "--linkage": "",
        "threads": ""
    },
    "import_percent_of_reads_mapped": {
        "run": true,
        "threads": ""
    },
    "krakenuniq": {
        "threads": 3,
        "--gzip-compressed": true,
        "additional_params": "",
        "run": "",
        "--db": "/scratch/genomics/scottjj/kraken_dbs/DB/"
    },
    "remove_short_reads_based_on_references": {
        "delimiter-for-iu-remove-ids-from-fastq": " ",
        "dont_remove_just_map": "",
        "references_for_removal_txt": "",
        "threads": ""
    },
    "anvi_cluster_contigs": {
        "--collection-name": "{driver}",
        "run": "",
        "--driver": "concoct",
        "--just-do-it": "",
        "--additional-params-concoct": "",
        "--additional-params-metabat2": "",
        "--additional-params-maxbin2": "",
        "--additional-params-dastool": "",
        "--additional-params-binsanity": "",
        "threads": 10
    },
    "gen_external_genome_file": {
        "threads": ""
    },
    "export_gene_calls_for_centrifuge": {
        "threads": ""
    },
    "anvi_import_taxonomy_for_genes": {
        "threads": ""
    },
    "annotate_contigs_database": {
        "threads": ""
    },
    "anvi_get_sequences_for_gene_calls": {
        "threads": ""
    },
    "gunzip_fasta": {
        "threads": ""
    },
    "reformat_external_gene_calls_table": {
        "threads": ""
    },
    "reformat_external_functions": {
        "threads": ""
    },
    "import_external_functions": {
        "threads": ""
    },
    "anvi_run_pfams": {
        "run": true,
        "--pfam-data-dir": "/pool/genomics/stri_istmobiome/dbs/pfam_db",
        "threads": 5
    },
    "iu_gen_configs": {
        "--r1-prefix": "",
        "--r2-prefix": "",
        "threads": ""
    },
    "gen_qc_report": {
        "threads": ""
    },
    "merge_fastqs_for_co_assembly": {
        "threads": ""
    },
    "merge_fastas_for_co_assembly": {
        "threads": ""
    },
    "bowtie_build": {
        "threads": ""
    },
    "anvi_init_bam": {
        "threads": ""
    },
    "krakenuniq_mpa_report": {
        "threads": ""
    },
    "import_krakenuniq_taxonomy": {
        "--min-abundance": "",
        "threads": ""
    },
    "anvi_summarize": {
        "additional_params": "",
        "run": "",
        "threads": ""
    },
    "anvi_split": {
        "additional_params": "",
        "run": "",
        "threads": ""
    },
    "references_mode": "",
    "all_against_all": "",
    "kraken_txt": "",
    "collections_txt": "",
    "output_dirs": {
        "FASTA_DIR": "02_FASTA",
        "CONTIGS_DIR": "03_CONTIGS",
        "QC_DIR": "01_QC",
        "MAPPING_DIR": "04_MAPPING",
        "PROFILE_DIR": "05_ANVIO_PROFILE",
        "MERGE_DIR": "06_MERGED",
        "TAXONOMY_DIR": "07_TAXONOMY",
        "SUMMARY_DIR": "08_SUMMARY",
        "SPLIT_PROFILES_DIR": "09_SPLIT_PROFILES",
        "LOGS_DIR": "00_LOGS"
    },
    "max_threads": "",
    "config_version": "1"
}
</code></pre>
</details>
<p><br/></p>
<p>Since we are doing a co-assembly, we need a separate file called <code>samples.txt</code>, which can be downloaded <a href="files/samples.txt">here</a>. This file tells anvi’o where to find the <strong>trimmed</strong> fastq files for each sample and what group the sample belongs to. The file is a four-column, tab-delimited file.</p>
<div class="l-body-outset">
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 7%" />
<col style="width: 41%" />
<col style="width: 41%" />
</colgroup>
<thead>
<tr class="header">
<th>sample</th>
<th>group</th>
<th>r1</th>
<th>r2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>WCCR_1913</td>
<td>WATER</td>
<td>comma separated list of forward reads</td>
<td>comma separated list of reverse reads</td>
</tr>
<tr class="even">
<td>WCCR_1916</td>
<td>WATER</td>
<td>comma separated list of forward reads</td>
<td>comma separated list of reverse reads</td>
</tr>
<tr class="odd">
<td>WROL_1914</td>
<td>WATER</td>
<td>comma separated list of forward reads</td>
<td>comma separated list of reverse reads</td>
</tr>
<tr class="even">
<td>WROL_1915</td>
<td>WATER</td>
<td>comma separated list of forward reads</td>
<td>comma separated list of reverse reads</td>
</tr>
</tbody>
</table>
</div>
<p>Remember, there are 4 fastq files <em>per</em> direction (forward or reverse) <em>per</em> sample. So each comma separated list in the <code>r1</code> column needs four file names and the same with the <code>r2</code> column. And you must include <em>relative path names</em>.</p>
<p>To see what will happen with the <code>config</code> file we can visualize the Snakemake workflow using a Directed acyclic graph (DAG) where edge connections represent dependencies and nodes represent commands. The workflow begins with trimmed reads and continues up to and including automatic binning of contigs. At the end of this workflow we then proceed with manual binning and MAG generation. We added nodes for Virsorter and Kaiju annotations since these are not part of the normal anvi’o workflow.</p>
<blockquote>
<p>Click on the image to zoom in or download a copy.</p>
</blockquote>
<details>
<summary>Show/hide DAG R Code script</summary>
<pre><code>
<div class="layout-chunk" data-layout="l-body">

```r
dag <- grViz ("
digraph boxes_and_circles {
  graph [layout = dot, align=center]

  node [shape = rectangle, style = 'rounded,filled' fontname=sans, fontsize=12, penwidth=4]
  edge[penwidth=4, color=grey];

0[label = 'metagenomics_workflow_target_rule', color = 'grey'];
1[label = 'anvi_merge', color = '#CC79A7'];
2[label = 'anvi_merge', color = '#CC79A7'];
3[label = 'annotate_contigs_database', color = '#E69F00'];
4[label = 'annotate_contigs_database', color = '#E69F00'];
5[label = 'gen_qc_report', color = '#56B4E9'];
6[label = 'anvi_cluster_contigs\ndriver: concoct', color = '#F0E442'];
7[label = 'anvi_cluster_contigs\ndriver: concoct', color = '#F0E442'];
8[label = 'anvi_gen_contigs_database', color = '#CC79A7'];
9[label = 'anvi_profile', color = '#CC79A7'];
10[label = 'anvi_profile', color = '#CC79A7'];
11[label = 'import_percent_of_reads_mapped', color = '#009E73'];
12[label = 'import_percent_of_reads_mapped', color = '#009E73'];
13[label = 'import_krakenuniq_taxonomy', color = '#E69F00'];
14[label = 'import_krakenuniq_taxonomy', color = '#E69F00'];
15[label = 'anvi_gen_contigs_database', color = '#CC79A7'];
16[label = 'anvi_profile', color = '#CC79A7'];
17[label = 'anvi_profile', color = '#CC79A7'];
18[label = 'import_percent_of_reads_mapped', color = '#009E73'];
19[label = 'import_percent_of_reads_mapped', color = '#009E73'];
20[label = 'import_krakenuniq_taxonomy', color = '#E69F00'];
21[label = 'import_krakenuniq_taxonomy', color = '#E69F00'];
22[label = 'anvi_import_taxonomy_for_genes', color = '#E69F00'];
23[label = 'anvi_run_hmms', color = '#E69F00'];
24[label = 'anvi_run_ncbi_cogs', color = '#E69F00'];
25[label = 'anvi_run_scg_taxonomy', color = '#E69F00'];
26[label = 'anvi_run_pfams', color = '#E69F00'];
27[label = 'anvi_import_taxonomy_for_genes', color = '#E69F00'];
28[label = 'anvi_run_hmms', color = '#E69F00'];
29[label = 'anvi_run_ncbi_cogs', color = '#E69F00'];
30[label = 'anvi_run_scg_taxonomy', color = '#E69F00'];
31[label = 'anvi_run_pfams', color = '#E69F00'];
32[label = 'iu_filter_quality_minoche\nsample: WCCR_1913', color = '#56B4E9'];
33[label = 'iu_filter_quality_minoche\nsample: WCCR_1916', color = '#56B4E9'];
34[label = 'iu_filter_quality_minoche\nsample: WROL_1914', color = '#56B4E9'];
35[label = 'iu_filter_quality_minoche\nsample: WROL_1915', color = '#56B4E9'];
36[label = 'anvi_script_reformat_fasta', color = '#56B4E9'];
37[label = 'anvi_init_bam', color = '#009E73'];
38[label = 'anvi_init_bam', color = '#009E73'];
39[label = 'krakenuniq_mpa_report', color = '#E69F00'];
40[label = 'krakenuniq_mpa_report', color = '#E69F00'];
41[label = 'anvi_script_reformat_fasta', color = '#56B4E9'];
42[label = 'anvi_init_bam', color = '#009E73'];
43[label = 'anvi_init_bam', color = '#009E73'];
44[label = 'krakenuniq_mpa_report', color = '#E69F00'];
45[label = 'krakenuniq_mpa_report', color = '#E69F00'];
46[label = 'centrifuge', color = '#E69F00'];
47[label = 'centrifuge', color = '#E69F00'];
48[label = 'iu_gen_configs', color = '#56B4E9'];
49[label = 'anvi_script_reformat_fasta_prefix_only', color = '#56B4E9'];
50[label = 'samtools_view', color = '#009E73'];
51[label = 'samtools_view', color = '#009E73'];
52[label = 'krakenuniq', color = '#E69F00'];
53[label = 'krakenuniq', color = '#E69F00'];
54[label = 'anvi_script_reformat_fasta_prefix_only', color = '#56B4E9'];
55[label = 'samtools_view', color = '#009E73'];
56[label = 'samtools_view', color = '#009E73'];
57[label = 'krakenuniq', color = '#E69F00'];
58[label = 'krakenuniq', color = '#E69F00'];
59[label = 'export_gene_calls_for_centrifuge', color = '#E69F00'];
60[label = 'export_gene_calls_for_centrifuge', color = '#E69F00'];
61[label = 'megahit\ngroup: EP', color = '#56B4E9'];
62[label = 'bowtie', color = '#009E73'];
63[label = 'bowtie', color = '#009E73'];
64[label = 'gzip_fastqs\nR: R1', color = '#56B4E9'];
65[label = 'gzip_fastqs\nR: R2', color = '#56B4E9'];
66[label = 'gzip_fastqs\nR: R1', color = '#56B4E9'];
67[label = 'gzip_fastqs\nR: R2', color = '#56B4E9'];
68[label = 'megahit\ngroup: WA', color = '#56B4E9'];
69[label = 'bowtie', color = '#009E73'];
70[label = 'bowtie', color = '#009E73'];
71[label = 'gzip_fastqs\nR: R1', color = '#56B4E9'];
72[label = 'gzip_fastqs\nR: R2', color = '#56B4E9'];
73[label = 'gzip_fastqs\nR: R1', color = '#56B4E9'];
74[label = 'gzip_fastqs\nR: R2', color = '#56B4E9'];
75[label = 'bowtie_build', color = '#009E73'];
76[label = 'bowtie_build', color = '#009E73'];
77[label = 'virsorter', color = '#E69F00', style = 'dashed'];
78[label = 'virsorter', color = '#E69F00', style = 'dashed'];
79[label = 'kaiju', color = '#E69F00', style = 'dashed'];
80[label = 'kaiju', color = '#E69F00', style = 'dashed'];

1->0; 2->0; 3->0; 4->0; 5->0; 6->0; 7->0; 8->1; 9->1;
10->1; 11->1; 12->1; 13->1; 14->1; 15->2; 16->2; 17->2;
18->2; 19->2; 20->2; 21->2; 8->3; 22->3; 23->3; 24->3;
25->3; 26->3; 15->4; 27->4; 28->4; 29->4; 30->4; 31->4;
{32 33 34 35}->5;
48->{32 33 34 35};
32->{64 65};
33->{66 67};
{64 65 66 67}->61;
{64 65}->52;
{66 67}->53;
34->{71 72};
35->{73 74};
{71 72 73 74}->68
{71 72}->57;
{73 74}->58;

8->6; 1->6; 15->7; 2->7;
36->8; 37->9; 8->9; 38->10; 8->10; 9->11; 10->12; 39->13;
11->13; 9->13; 40->14; 12->14; 10->14; 41->15; 42->16; 15->16;
43->17; 15->17; 16->18; 17->19; 44->20; 18->20; 16->20; 45->21;
19->21; 17->21; 46->22; 8->22; 8->23; 8->24; 23->25; 8->25;
8->26; 47->27; 15->27; 15->28; 15->29; 28->30; 15->30; 15->31;
49->36; 50->37; 51->38; 52->39;
53->40; 54->41; 55->42; 56->43; 57->44; 58->45; 59->46; 60->47;
61->49; 62->50; 63->51; 68->54;
69->55; 70->56; 8->59; 15->60;
75->62; 64->62; 65->62; 75->63;
66->63; 67->63; 76->69; 71->69; 72->69; 76->70; 73->70; 74->70;
36->75; 41->76; 79->22; 80->27;
15->77; 8->78; 77->4; 78->3; 59->79; 60->80;

    graph [nodesep = 0.1]
{ rank=same; 13, 14, 20, 21 }
{ rank=same; 39, 40, 44, 45 }
{ rank=same; 32, 33, 34, 35 }
{ rank=same; 64, 65, 66, 67, 71, 72, 73, 74 }
{ rank=same; 5, 49, 54 }

}
")

export_svg(dag) %>%
  charToRaw() %>%
  rsvg() %>%
  png::writePNG("images/dag.png")
```

</div>

</code></pre>
</details>
<div class="l-body-outset">
<figure>
<a href="images/dag.png"> <img src="images/dag.png" class="thumbnail"> </a>
<figcaption>
<strong>DAG of the metagenomic workflow</strong>. Colors indicate broad divisions of workflow: sky blue, short-read prep &amp; co-assembly; blueish green, short-read mapping to assembly; orange, taxonomic or functional classification; yellow, automatic binning; reddish purple, databases construction.
</figcaption>
</figure>
</div>
<p>And here are the commands we used to run the workflow.</p>
<pre class="bash"><code>
anvi-run-workflow -w metagenomics -c default-config.txt --additional-params --jobs 63 --resources nodes=63 --keep-going --rerun-incomplete --unlock
anvi-run-workflow -w metagenomics -c default-config.txt --additional-params --jobs 63 --resources nodes=63 --keep-going --rerun-incomplete</code></pre>
<p>If a Snakemake job fails the defualt behavior is to lock the workflow and because large jobs can fail for a lot of reasons, we decided to always include an identical command first with the <code>--unlock</code> flag. This will unlock the workflow and then the second command will execute. You can find an explaination of this <a href="http://merenlab.org/2018/07/09/anvio-snakemake-workflows/#how-can-i-restart-a-failed-job">here</a>.</p>
<details>
<summary>Show/hide HYDRA SNAKEMAKE job script</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 20
#$ -q sThM.q
#$ -l mres=140G,h_data=7G,h_vmem=7G,himem
#$ -cwd
#$ -j y
#$ -N job_01_run_mg_workflow
#$ -o hydra_logs/job_01_run_mg_workflow.log
#
# ----------------Modules------------------------- #
module load gcc/4.9.2
#
# ----------------Your Commands------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
# ----------------CALLING ANVIO------------------- #
export PATH=/home/scottjj/miniconda3:$PATH
export PATH=/home/scottjj/miniconda3/bin:$PATH
source activate anvio-master
#
# ----------------COOLIO?------------------- #
which python
python --version
source /home/scottjj/virtual-envs/anvio-master/bin/activate
which python
python --version
which anvi-interactive
diamond --version
anvi-self-test -v
#
# ----------------TEMP DIRECTORIES------------------- #
rm -r /pool/genomics/stri_istmobiome/dbs/cog_db/tmp/
mkdir -p /pool/genomics/stri_istmobiome/dbs/cog_db/tmp/
#
rm -r /pool/genomics/stri_istmobiome/dbs/pfam_db/tmp_data/
mkdir -p /pool/genomics/stri_istmobiome/dbs/pfam_db/tmp_data/
TMPDIR="/pool/genomics/stri_istmobiome/dbs/pfam_db/tmp_data/"
#
# ----------------COMMANDS------------------- #
#
anvi-run-workflow -w metagenomics -c default-config.txt --additional-params --jobs 20 --resources nodes=20 --keep-going --rerun-incomplete --unlock
anvi-run-workflow -w metagenomics -c default-config.txt --additional-params --jobs 20 --resources nodes=20 --keep-going --rerun-incomplete
echo = `date` job $JOB_NAME don
</code></pre>
</details>
<h4 id="snakemake-citations">Snakemake Citations</h4>
<p>There are many tools used in the workflow that need to be cited.</p>
<p>First there is the workflow engine itself—Snakemake<span class="citation" data-cites="koster2012snakemake">(Köster and Rahmann <a href="#ref-koster2012snakemake">2012</a>)</span>. QUALITY-FILTERING of raw reads was performed using Illumina Utils<span class="citation" data-cites="eren2013filtering">(Eren et al. <a href="#ref-eren2013filtering">2013</a>)</span>. CO-ASSEMBLY of metagenomic samples performed using MEGAHIT<span class="citation" data-cites="li2015megahit">(Li et al. <a href="#ref-li2015megahit">2015</a>)</span>. GENE CALLING was performed using PRODIGAL<span class="citation" data-cites="hyatt2010prodigal">(Hyatt et al. <a href="#ref-hyatt2010prodigal">2010</a>)</span>. RECRUITMENT (a.k.a. mapping) of reads to assembly performed using BOWTIE2<span class="citation" data-cites="langmead2012fast">(Langmead and Salzberg <a href="#ref-langmead2012fast">2012</a>)</span> and SAMtools<span class="citation" data-cites="li2009sequence">(Li et al. <a href="#ref-li2009sequence">2009</a>)</span>. TAXONOMIC CLASSIFICATION of genes using CENTRIFUGE<span class="citation" data-cites="kim2016centrifuge">(Kim et al. <a href="#ref-kim2016centrifuge">2016</a>)</span>.</p>
<aside>
Documentation for <a href="https://snakemake.readthedocs.io/en/stable/">Snakemake</a>. GitHub repo for <a href="https://github.com/merenlab/illumina-utils">Illumina Utils</a>. GitHub repo for <a href="https://github.com/voutcn/megahit">MEGAHIT</a>. GitHub repo for <a href="https://github.com/hyattpd/Prodigal">PRODIGAL</a>. GitHub repo for <a href="https://github.com/BenLangmead/bowtie2">BOWTIE2</a>. Documentation for <a href="http://samtools.sourceforge.net/">SAMtools</a> GitHub repo for <a href="https://github.com/DaehwanKimLab/centrifuge">CENTRIFUGE</a>.
</aside>
<p>Also, there are a few tools that we ran outside of the Snakemake workflow. Results from these steps need to be added to the individual <code>PROFILE.db</code>’s, merged <code>PROFILE.db</code>, or <code>CONTIGS.db</code>. Therefore, before the <code>anvi-merge</code> portion of the Snakemake workflow finished, we killed the job, ran the accessory analyses described below, and then restarted the workflow to finish the missing step. Cumbersome, yes, but it got the job done.</p>
<h2 id="taxonomic-annotations">Taxonomic Annotations</h2>
<p>In this section we conduct taxonomic classification of short reads, contigs, and gene calls.</p>
<h3 id="kaken-annotation">Kaken Annotation</h3>
<p>In this section we use KrakenUniq<span class="citation" data-cites="breitwieser2018krakenuniq">(Breitwieser, Baker, and Salzberg <a href="#ref-breitwieser2018krakenuniq">2018</a>)</span> to classify the <strong>short reads</strong>. Because of the memory demands of KrakenUniq, we could not get this to work in the Snakemake workflow so we ran the analysis separately. For several of these commands we used lists of sample names and <code>for</code> loops to run through each sample.</p>
<aside>
GitHub repo for <a href="https://github.com/fbreitwieser/krakenuniq">KrakenUniq</a>.
</aside>
<p>Ok, here we run KrakenUniq and several reporting and summary steps against the short read data for each sample using the <a href="files/list.txt">list.txt</a> file.</p>
<pre class="bash"><code>
for sample in `cat list.txt`
do
    krakenuniq --report-file $KRAKEN/$sample-REPORT.tsv 01_QC/$sample-QUALITY_PASSED_R1.fastq.gz 01_QC/$sample-QUALITY_PASSED_R2.fastq.gz --db $K_FILES/DB/ --threads $NSLOTS --preload  --fastq-input --gzip-compressed --paired --output $KRAKEN/$sample-kraken.out

    krakenuniq-report --db $K_FILES/DB/ $KRAKEN/$sample-kraken.out &gt; $KRAKEN/$sample-kraken_report.txt
    krakenuniq-mpa-report --header-line --db $K_FILES/DB/ $KRAKEN/$sample-kraken.out &gt; $KRAKEN/$sample-kraken_mpa_report.txt
    krakenuniq-translate --db $K_FILES/DB/ $KRAKEN/$sample-kraken.out &gt; $KRAKEN/$sample-kraken.trans
done</code></pre>
<p>Next, we format the output to make Krona plots.</p>
<pre class="bash"><code>
for sample in `cat list.txt`
do
    $KRA_to_KRON/kraken_to_krona.py $KRAKEN/$sample-kraken.trans &gt; $KRAKEN/$sample-kraken.krona
done</code></pre>
<p>And finally construct Krona<span class="citation" data-cites="ondov2011krona">(Ondov, Bergman, and Phillippy <a href="#ref-ondov2011krona">2011</a>)</span> plots. We will come back to Krona plots when we explain what they are and how to use them in the <a href="mg-workflow-2.html#krona-plots-explained">Assembley &amp; Annotation Summary</a> section of the workflow.</p>
<aside>
<a href="https://github.com/marbl/Krona/wiki">GitHub Repo for Krona</a>.
</aside>
<pre class="bash"><code>
for sample in `cat list.txt`
do
    ktImportText -o $KRAKEN/$sample-kraken.html $KRAKEN/$sample-kraken.krona
done</code></pre>
<p>We can also make standalone HTML pages from each assembly containing all Krona plots.</p>
<pre class="bash"><code>
ktImportText -o $KRAKEN/WATER-kraken.html $KRAKEN/WCCR_1913-kraken.krona $KRAKEN/WCCR_1916-kraken.krona $KRAKEN/WROL_1914-kraken.krona $KRAKEN/WROL_1915-kraken.krona</code></pre>
<p>Or add the <code>-c</code> flag to make a single plot where samples are combined.</p>
<p>Now we have a Krona plot page for the water sample.</p>
<details>
<summary>Show/hide HYDRA Kraken classification job details</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 3
#$ -q mThM.q
#$ -l mres=150G,h_data=150G,h_vmem=150G,himem
#$ -cwd
#$ -j y
#$ -N job_04_taxonomic_classification_kraken_reads
#$ -o hydra_logs/job_04_taxonomic_classification_kraken_reads.job
#
# ----------------Modules------------------------- #
#
# ----------------Load Envs------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
# ----------------Activate Kraken -------------- #
#
export PATH=/home/scottjj/miniconda3/bin:$PATH
source activate kraken
# ----------------SETUP Kraken Directories-------------- #
#
mkdir 07_TAXONOMY/KRAKEN/
mkdir 07_TAXONOMY/KRAKEN/READS
#
KRAKEN='/pool/genomics/stri_istmobiome/data/HYPOXIA_DATA/HYPOXIA/07_TAXONOMY/KRAKEN_TAXONOMY/MICROBIAL-NT/'
K_FILES='/scratch/genomics/scottjj/kraken_dbs/microbial-nt/'
KRA_to_KRON='/home/scottjj/miniconda3/envs/metawrap/bin/metawrap-scripts/'
# ----------------Run Kraken------------------- #
#
for sample in `cat list.txt`
do
    krakenuniq --report-file $KRAKEN/$sample-REPORT.tsv 01_QC/$sample-QUALITY_PASSED_R1.fastq.gz 01_QC/$sample-QUALITY_PASSED_R2.fastq.gz --db $K_FILES/DB/ --threads $NSLOTS --preload  --fastq-input --gzip-compressed --paired --output $KRAKEN/$sample-kraken.out
    krakenuniq-report --db $K_FILES/DB/ $KRAKEN/$sample-kraken.out > $KRAKEN/$sample-kraken_report.txt
    krakenuniq-mpa-report --header-line --db $K_FILES/DB/ $KRAKEN/$sample-kraken.out > $KRAKEN/$sample-kraken_mpa_report.txt
    krakenuniq-translate --db $K_FILES/DB/ $KRAKEN/$sample-kraken.out > $KRAKEN/$sample-kraken.trans
done
source deactivate
#
source deactivate
#
# ----------------FORMAT FOR KRONA------------------- #
#
source activate metawrap
#
for sample in `cat list.txt`
do
    $KRA_to_KRON/kraken_to_krona.py $KRAKEN/$sample-kraken.trans > $KRAKEN/$sample-kraken.krona
done
source deactivate
#
# ----------------MAKE KRONA PLOTS------------------- #
#
source activate krona
#
for sample in `cat list.txt`
#
do
    ktImportText -o $KRAKEN/$sample-kraken.html $KRAKEN/$sample-kraken.krona
done
#
ktImportText -o $KRAKEN/WATER-kraken.html $KRAKEN/WCCR_1913-kraken.krona $KRAKEN/WCCR_1916-kraken.krona $KRAKEN/WROL_1914-kraken.krona $KRAKEN/WROL_1915-kraken.krona
#
source deactivate
#
echo = `date` job $JOB_NAME don
</code></pre>
</details>
<h4 id="kraken-import">Kraken Import</h4>
<p>At this point we now add the kraken summary annotations to each individual <code>PROFILE.db</code> created in the Snakemake workflow. Again we used lists of sample names and <code>for</code> loops to run through each sample.</p>
<pre class="bash"><code>
for sample in `cat list.txt`
do
    anvi-import-taxonomy-for-layers -p 05_ANVIO_PROFILE/WATER/$sample/PROFILE.db --parse krakenuniq -i 07_TAXONOMY/KRAKEN_TAXONOMY/MICROBIAL-NT/$sample-kraken_mpa_report.txt
done</code></pre>
<details>
<summary>Show/hide HYDRA Import Kraken taxonomy job details</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -q sThC.q
#$ -l mres=5G,h_data=5G,h_vmem=5G
#$ -cwd
#$ -j y
#$ -N job_06_import_taxonomy_for_layers
#$ -o hydra_logs/job_06_import_taxonomy_for_layers.log
#
# ----------------Modules------------------------- #
module load gcc/4.9.2
#
# ----------------Your Commands------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
# ----------------Activate Anvio -------------- #
#
export PATH=/home/scottjj/miniconda3:$PATH
export PATH=/home/scottjj/miniconda3/bin:$PATH
source activate anvio-master
#
which python
python --version
source /home/scottjj/virtual-envs/anvio-master/bin/activate
which python
python --version
which anvi-interactive
diamond --version
anvi-self-test -v
#
# ----------------Import Kraken Annotations------------------- #
#
for sample in `cat list.txt`
#
do
    anvi-import-taxonomy-for-layers -p 05_ANVIO_PROFILE/WATER/$sample/PROFILE.db --parse krakenuniq -i 07_TAXONOMY/KRAKEN_TAXONOMY/MICROBIAL-NT/$sample-kraken_mpa_report.txt
done
#
source deactivate
#
echo = `date` job $JOB_NAME don
</code></pre>
</details>
<p><br/></p>
<p>After this you need to rerun <code>anvi-merge</code> so that the short read annotations make it into the merged <code>PROFILE.db</code>. But we will get to that in a minute.</p>
<h3 id="virsorter-annotation">VirSorter Annotation</h3>
<p>To classify any viral sequences, we ran Virsorter<span class="citation" data-cites="roux2015virsorter">(Roux et al. <a href="#ref-roux2015virsorter">2015</a>)</span> on conigs from the co-assembly using our newly created <code>contig.db</code> generated after the co-assembly step. We ran Virsorter with and without the <code>--virome</code> flag. See <a href="https://github.com/simroux/VirSorter/issues/40">issue 40</a> on the Virsorter GitHub site for a discussion of why we did this. We used DIAMOND<span class="citation" data-cites="buchfink2015fast">(Buchfink, Xie, and Huson <a href="#ref-buchfink2015fast">2015</a>)</span> as the local sequence aligner.</p>
<aside>
<a href="https://github.com/simroux/VirSorter">GitHub Repo for VirSorter</a>. Documentation for <a href="http://www.diamondsearch.org/index.php">DIAMOND</a>.
</aside>
<p>Here is the VirSorter workflow.</p>
<pre class="bash"><code>
wrapper_phage_contigs_sorter_iPlant.pl -f 02_FASTA/WATER/WATER-contigs.fa --ncpu $NSLOTS --db 2 --wdir 07_TAXONOMY/VIRSORTER/WATER --data-dir /pool/genomics/stri_istmobiome/dbs/virsorter/virsorter-data --diamond
wrapper_phage_contigs_sorter_iPlant.pl -f 02_FASTA/WATER/WATER-contigs.fa --ncpu $NSLOTS --db 2 --wdir 07_TAXONOMY/VIRSORTER_VIROME/WATER --data-dir /pool/genomics/stri_istmobiome/dbs/virsorter/virsorter-data --diamond --virome</code></pre>
<details>
<summary>Show/hide HYDRA VIRSORTER job script</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 10
#$ -q sThC.q
#$ -l mres=50G,h_data=5G,h_vmem=5G
#$ -cwd
#$ -j y
#$ -N job_03_run_virsorter
#$ -o hydra_logs/job_03_run_virsorter.log
#
# ----------------Modules------------------------- #
module load gcc/4.9.2
#
# ----------------Your Commands------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
export PATH=/home/scottjj/miniconda3:$PATH
export PATH=/home/scottjj/miniconda3/bin:$PATH
# ----------------Calling Virsorter------------------- #
#
source activate virsorter
which perl
#
# ----------------Make directories------------------- #
mkdir 07_TAXONOMY/
mkdir 07_TAXONOMY/VIRSORTER/
#
# ----------------COMMANDS------------------- #
wrapper_phage_contigs_sorter_iPlant.pl -f 02_FASTA/WATER/WATER-contigs.fa --ncpu $NSLOTS --db 2 --wdir 07_TAXONOMY/VIRSORTER/WATER --data-dir /pool/genomics/stri_istmobiome/dbs/virsorter/virsorter-data --diamond
#
wrapper_phage_contigs_sorter_iPlant.pl -f 02_FASTA/WATER/WATER-contigs.fa --ncpu $NSLOTS --db 2 --wdir 07_TAXONOMY/VIRSORTER_VIROME/WATER --data-dir /pool/genomics/stri_istmobiome/dbs/virsorter/virsorter-data --diamond --virome
#
source deactivate
#
echo = `date` job $JOB_NAME don
</code></pre>
</details>
<h3 id="kaiju-annotation">Kaiju Annotation</h3>
<p>In addition to the Centrifuge classification we are going to use Kaiju<span class="citation" data-cites="menzel2016fast">(Menzel, Ng, and Krogh <a href="#ref-menzel2016fast">2016</a>)</span> to classify gene calls. We do this against the <a href="https://github.com/bioinformatics-centre/kaiju#creating-the-reference-database-and-index">two databases</a> we built on the <a href="mg-databases.html/#kaiju">Annotation Databases</a> page.</p>
<aside>
GitHub repo for <a href="https://github.com/bioinformatics-centre/kaiju">Kaiju</a>.
</aside>
<p>First we need to activate anvi’o, make a directory for the output, and grab the gene calls. Note that <code>$KAIJU</code> is the path to the input/output directory, which in this case is the new directory, <code>07_TAXONOMY/KAIJU/</code>.</p>
<pre class="bash"><code>
mkdir 07_TAXONOMY/KAIJU/
anvi-get-sequences-for-gene-calls -c 03_CONTIGS/WATER-contigs.db -o $KAIJU/WATER_gene_calls.fna</code></pre>
<p>Next we run the Kaiju commands against the <code>nr_euk</code> database and then the <code>mar</code> database. Note that <code>$K_FILES</code> is the path to the database and <code>$KAIJU</code> is the path to the input/output directory.</p>
<pre class="bash"><code>
# against nr_euk db
kaiju -t $K_FILES/nr_db/nodes.dmp -f $K_FILES/nr_db/nr_euk/kaiju_db_nr_euk.fmi -i $KAIJU/WATER_gene_calls.fna -o $KAIJU/WATER_kaiju_nr.out -z 16 -v
# against mar db
kaiju -t $K_FILES/marine_db/nodes.dmp -f $K_FILES/marine_db/marine_db/kaiju_db_mar.fmi -i $KAIJU/WATER_gene_calls.fna -o $KAIJU/WATER_kaiju_mar.out -z 16 -v</code></pre>
<p>Finally, we need to add taxon names to the output in order to import the taxonomy into the <code>contig.db</code> later on. We will also compare the output of the two classifications.</p>
<pre class="bash"><code>
# against nr_euk db
kaiju-addTaxonNames -t $K_FILES/nr_db/nodes.dmp -n $K_FILES/nr_db/names.dmp -i $KAIJU/WATER_kaiju_nr.out -o $KAIJU/WATER_kaiju_nr.names -r superkingdom,phylum,order,class,family,genus,species
# against mar db
kaiju-addTaxonNames -t $K_FILES/marine_db/nodes.dmp -n $K_FILES/marine_db/names.dmp -i $KAIJU/WATER_kaiju_mar.out -o $KAIJU/WATER_kaiju_mar.names -r superkingdom,phylum,order,class,family,genus,species</code></pre>
<details>
<summary>Show/hide HYDRA KAIJU job script</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 20
#$ -q sThC.q
#$ -l mres=120G,h_data=6G,h_vmem=6G
#$ -cwd
#$ -j y
#$ -N job_04_taxonomic_classification_kaiju
#$ -o hydra_logs/job_04_taxonomic_classification_kaiju.job
#
# ----------------Modules------------------------- #
#
# ----------------Load Envs------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
export PATH=/home/scottjj/miniconda3:$PATH
export PATH=/home/scottjj/miniconda3/bin:$PATH
export PATH=/home/scottjj/miniconda3/envs/kaiju/bin:$PATH
export PATH=/home/scottjj/miniconda3/envs/krona/bin:$PATH
export PATH=/pool/genomics/stri_istmobiome/dbs/kaiju_db/:$PATH
#
#NOT SURE IF THE FOLLOWING TWO LINES ARE NEEDED
export PERL5LIB="/home/scottjj/miniconda3/envs/kaiju/lib/5.26.2"
export PERL5LIB="/home/scottjj/miniconda3/envs/kaiju/lib/5.26.2/x86_64-linux-thread-multi:$PERL5LIB"
# ----------------SETUP KAIJU Directories-------------- #
#
mkdir 07_TAXONOMY/KAIJU/
KAIJU='/pool/genomics/stri_istmobiome/data/HYPOXIA_DATA/HYPOXIA/07_TAXONOMY/KAIJU/'
K_FILES='/pool/genomics/stri_istmobiome/dbs/kaiju_db'
#
# ----------------Get Gene Files------------------- #
source activate anvio-6.1
#
anvi-get-sequences-for-gene-calls -c 03_CONTIGS/WATER-contigs.db -o $KAIJU/WATER_gene_calls.fna
#
source deactivate
#
# ----------------RUN KAIJU------------------- #
source activate kaiju
which kaiju
gcc --version
which perl
# ----------------AGAINST nr DB------------------- #
#
kaiju -t $K_FILES/nr_db/nodes.dmp -f $K_FILES/nr_db/nr_euk/kaiju_db_nr_euk.fmi -i $KAIJU/WATER_gene_calls.fna -o $KAIJU/WATER_kaiju_nr.out -z 16 -v
#
kaiju-addTaxonNames -t $K_FILES/nr_db/nodes.dmp -n $K_FILES/nr_db/names.dmp -i $KAIJU/WATER_kaiju_nr.out -o $KAIJU/WATER_kaiju_nr.names -r superkingdom,phylum,order,class,family,genus,species
#
# ----------------AGAINST marine DB------------------- #
#
kaiju -t $K_FILES/marine_db/nodes.dmp -f $K_FILES/marine_db/marine_db/kaiju_db_mar.fmi -i $KAIJU/WATER_gene_calls.fna -o $KAIJU/WATER_kaiju_mar.out -z 16 -v
#
kaiju-addTaxonNames -t $K_FILES/marine_db/nodes.dmp -n $K_FILES/marine_db/names.dmp -i $KAIJU/WATER_kaiju_mar.out -o $KAIJU/WATER_kaiju_mar.names -r superkingdom,phylum,order,class,family,genus,species
#
source deactivate
#
echo = `date` job $JOB_NAME don
</code></pre>
</details>
<h4 id="kaiju-import">Kaiju Import</h4>
<p>Now its is time to add the Kaiju annotation to the <code>contig.db</code>. We will also use the opportunity to generate some <a href="https://github.com/marbl/Krona/wiki">Krona plots</a>. We will discuss these in more detail in the next chapter.</p>
<p>First we need to take the Kaiju-formmated output file and format it from Krona. We can do this for the results from both annotations.</p>
<pre class="bash"><code>
# from nr_euk db
kaiju2krona -t $K_FILES/nr_db/nodes.dmp -n $K_FILES/nr_db/names.dmp -i $KAIJU/WATER_kaiju_nr.out -o $KAIJU/WATER_kaiju_nr.out.krona
# from mar db
kaiju2krona -t $K_FILES/marine_db/nodes.dmp -n $K_FILES/marine_db/names.dmp -i $KAIJU/WATER_kaiju_mar.out -o $KAIJU/WATER_kaiju_mar.out.krona</code></pre>
<p>While we are at it, lets also generate some summary files of taxonomic content, again from both annotations.</p>
<pre class="bash"><code>
# from nr_euk db
kaiju2table -t $K_FILES/nr_db/nodes.dmp -n $K_FILES/nr_db/names.dmp -r class $KAIJU/WATER_kaiju_nr.out -l phylum,class,order,family -o $KAIJU/WATER_kaiju_nr.out.summary
# from mar db
kaiju2table -t $K_FILES/marine_db/nodes.dmp -n $K_FILES/marine_db/names.dmp -r class $KAIJU/WATER_kaiju_mar.out -l phylum,class,order,family -o $KAIJU/WATER_kaiju_mar.out.summary</code></pre>
<p>Now we will the output files from above to generate Krona plots for each annotation and send the output to HTML files. For this we use the krona conda package.</p>
<pre class="bash"><code>
# from nr_euk db
ktImportText -o $KAIJU/WATER_kaiju_nr.out.html $KAIJU/WATER_kaiju_nr.out.krona
#from mar db
ktImportText -o $KAIJU/WATER_kaiju_mar.out.html $KAIJU/WATER_kaiju_mar.out.krona</code></pre>
<p>Finally, it is time to import the Kaiju taxonomies into the <code>contig.db</code> for each assembly. As we will see later, the <code>mar db</code> annotations called very few viral sequences, even though we know from Virsorter there are a lot of viruses. Therefore we decided to use the <code>nr db</code> annotations.</p>
<pre class="bash"><code>
anvi-import-taxonomy-for-genes -c 03_CONTIGS/WATER-contigs.db -p kaiju -i $KAIJU/WATER_kaiju_nr.names --just-do-it</code></pre>
<details>
<summary>Show/hide HYDRA KAIJU import and summarize job script</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 5
#$ -q sThC.q
#$ -l mres=25G,h_data=5G,h_vmem=5G
#$ -cwd
#$ -j y
#$ -N job_05_kaiju_summary
#$ -o hydra_logs/job_05_kaiju_summary.job
#
# ----------------Modules------------------------- #
#
# ----------------Load Envs------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
export PATH=/home/scottjj/miniconda3:$PATH
export PATH=/home/scottjj/miniconda3/bin:$PATH
export PATH=/home/scottjj/miniconda3/envs/kaiju/bin:$PATH
export PATH=/home/scottjj/miniconda3/envs/krona/bin:$PATH
export PATH=/pool/genomics/stri_istmobiome/dbs/kaiju_db/:$PATH
#
#NOT SURE IF THE FOLLOWING TWO LINES ARE NEEDED
export PERL5LIB="/home/scottjj/miniconda3/envs/kaiju/lib/5.26.2"
export PERL5LIB="/home/scottjj/miniconda3/envs/kaiju/lib/5.26.2/x86_64-linux-thread-multi:$PERL5LIB"
# ----------------SETUP KAIJU Directories-------------- #
#
KAIJU='/pool/genomics/stri_istmobiome/data/HYPOXIA_DATA/HYPOXIA/07_TAXONOMY/KAIJU/'
K_FILES='/pool/genomics/stri_istmobiome/dbs/kaiju_db/'
#
# ----------------Activate------------------- #
source activate kaiju
which kaiju
gcc --version
which perl
# ----------------get files for KRONA plots------------------- #
#
# ----------------nr_euk db------------------- #
kaiju2krona -t $K_FILES/nr_db/nodes.dmp -n $K_FILES/nr_db/names.dmp -i $KAIJU/WATER_kaiju_nr.out -o $KAIJU/WATER_kaiju_nr.out.krona
#
kaiju2table -t $K_FILES/nr_db/nodes.dmp -n $K_FILES/nr_db/names.dmp -r class $KAIJU/WATER_kaiju_nr.out -l phylum,class,order,family -o $KAIJU/WATER_kaiju_nr.out.summary
#
# ----------------mar db------------------- #
#
kaiju2krona -t $K_FILES/marine_db/nodes.dmp -n $K_FILES/marine_db/names.dmp -i $KAIJU/WATER_kaiju_mar.out -o $KAIJU/WATER_kaiju_mar.out.krona
#
kaiju2table -t $K_FILES/marine_db/nodes.dmp -n $K_FILES/marine_db/names.dmp -r class $KAIJU/WATER_kaiju_mar.out -l phylum,class,order,family -o $KAIJU/WATER_kaiju_mar.out.summary
#
# ----------------Build KRONA plots------------------- #
source activate krona
ktImportText -o $KAIJU/WATER_kaiju_nr.out.html $KAIJU/WATER_kaiju_nr.out.krona
#
ktImportText -o $KAIJU/WATER_kaiju_mar.out.html $KAIJU/WATER_kaiju_mar.out.krona
conda deactivate
#--------------------ANVIO PARSER--------------#
source activate anvio-master
#
which python
python --version
source /home/scottjj/virtual-envs/anvio-master/bin/activate
which python
python --version
which anvi-interactive
diamond --version
anvi-self-test -v
#
anvi-import-taxonomy-for-genes -c 03_CONTIGS/WATER-contigs.db -p kaiju -i $KAIJU/WATER_kaiju_nr.names --just-do-it
#
source deactivate
#
echo = `date` job $JOB_NAME don
</code></pre>
</details>
<h2 id="functional-annotations">Functional Annotations</h2>
<h3 id="scg-taxonomy-pfam">SCG Taxonomy &amp; Pfam</h3>
<p>Here we run single-copy core genes in the <code>contigs.db</code> with taxonomic names against a local SCG taxonomy database. After this we can run <code>anvi-estimate-scg-taxonomy</code> to estimate taxonomy at genome-, collection-, or metagenome-level. We also needed to rerun the Pfam analysis because the original workflow failed at this step because of an improperly entered db path.</p>
<pre class="bash"><code>
# Against SCG
anvi-run-scg-taxonomy -c 03_CONTIGS/WAATER-contigs.db -P 1 -T $NSLOTS
# Against Pfam
anvi-run-pfams -c 03_CONTIGS/WATER-contigs.db --pfam-data-dir $PATH/pfam_db/ -T $NSLOTS</code></pre>
<details>
<summary>Show/hide HYDRA SCG &amp; Pfam job details</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 20
#$ -q mThM.q
#$ -l mres=200G,h_data=10G,h_vmem=10G,himem
#$ -cwd
#$ -j y
#$ -N job_07_run_scg_tax_and_pfam
#$ -o hydra_logs/job_07_run_scg_tax_and_pfam.log
#
# ----------------Modules------------------------- #
module load gcc/4.9.2
#
# ----------------Your Commands------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
# ----------------Activate Anvio -------------- #
#
export PATH=/home/scottjj/miniconda3:$PATH
export PATH=/home/scottjj/miniconda3/bin:$PATH
source activate anvio-master
#
which python
python --version
source /home/scottjj/virtual-envs/anvio-master/bin/activate
which python
python --version
which anvi-interactive
diamond --version
anvi-self-test -v
#
# ----------------Setup tmp directories------------------- #
rm -r /pool/genomics/stri_istmobiome/dbs/scgs-taxonomy-data/tmp_data/
mkdir -p /pool/genomics/stri_istmobiome/dbs/scgs-taxonomy-data/tmp_data/
TMPDIR="/pool/genomics/stri_istmobiome/dbs/scgs-taxonomy-data/tmp_data/"
#
rm -r /pool/genomics/stri_istmobiome/dbs/pfam_db/tmp_data/
mkdir -p /pool/genomics/stri_istmobiome/dbs/pfam_db/tmp_data/
TMPDIR="/pool/genomics/stri_istmobiome/dbs/pfam_db/tmp_data/"
#
# ----------------run anvio commands------------------- #
#
anvi-run-scg-taxonomy -c 03_CONTIGS/WATER-contigs.db -P 1 -T $NSLOTS
#
anvi-run-pfams -c 03_CONTIGS/WAATER-contigs.db --pfam-data-dir /pool/genomics/stri_istmobiome/dbs/pfam_db/ -T $NSLOTS
#
echo = `date` job $JOB_NAME done
</code></pre>
</details>
<h3 id="ghostkoala-annotations">GhostKOALA Annotations</h3>
<p>The last thing we can do is run KEGG annotations using the <a href="http://www.kegg.jp/ghostkoala/">GhostKOALA server</a>. Running GhostKOALA/KEGG was pretty easy thanks to this <a href="http://merenlab.org/2018/01/17/importing-ghostkoala-annotations/">handy tutorial</a>.</p>
<p>The steps we used to annotate genes with GhostKOALA/KEGG are reproduced from the tutorial.</p>
<p>A little setup first. Make a directory for the analysis and grab the parsing script.</p>
<pre class="bash"><code>
mkdir -p GhostKOALA/
git clone https://github.com/edgraham/GhostKoalaParser.git</code></pre>
<p>Now we need to export anvio gene calls as amino acid sequences.</p>
<pre class="bash"><code>
anvi-get-sequences-for-gene-calls -c 03_CONTIGS/WATER-contigs.db --get-aa-sequences -o GhostKOALA/WATER-protein-sequences.fa
anvi-get-sequences-for-gene-calls -c 03_CONTIGS/MAT-contigs.db --get-aa-sequences -o GhostKOALA/MAT-protein-sequences.fa</code></pre>
<p>Because of a peculiarity with the way the server handles fasta files we need to modify the fatsa headers. Anvio gene calls begin with a digit and the server doesn’t like that. So we will the prefix <code>genecall</code> to every gene call ID.</p>
<pre class="bash"><code>
sed -i &#39;s/&gt;/&gt;genecall_/&#39; GhostKOALA/WATER-protein-sequences.fa
sed -i &#39;s/&gt;/&gt;genecall_/&#39; GhostKOALA/MAT-protein-sequences.fa</code></pre>
<p>At this point we can upload our modified amino acid gene call files to the <a href="http://www.kegg.jp/ghostkoala/">GhostKOALA</a> server.</p>
<blockquote>
<p>You can only run one instance of GhostKOALA at a time per email address and the upload limit is 300Mb. You may need to split large fasta files in to smaller chunks.</p>
</blockquote>
<details>
<summary>Show/hide HYDRA GhostKOALA annotation import job script</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 20
#$ -q sThM.q
#$ -l mres=140G,h_data=7G,h_vmem=7G,himem
#$ -cwd
#$ -j y
#$ -N job_06_run_ghostkoala
#$ -o hydra_logs/job_01_run_ghostkoala.log
#$ -M scottjj@si.edu
#
# ----------------Modules------------------------- #
module load gcc/4.9.2
#
# ----------------Your Commands------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
# ----------------Activate Anvio -------------- #
#
export PATH=/home/scottjj/miniconda3:$PATH
export PATH=/home/scottjj/miniconda3/bin:$PATH
source activate anvio-master
#
which python
python --version
source /home/scottjj/virtual-envs/anvio-master/bin/activate
which python
python --version
which anvi-interactive
diamond --version
anvi-self-test -v
#
# ----------------Setup -------------- #
#
mkdir -p GhostKOALA/
git clone https://github.com/edgraham/GhostKoalaParser.git
# ----------------Export Anvio Gene Calls -------------- #
#
anvi-get-sequences-for-gene-calls -c 03_CONTIGS/WATER-contigs.db --get-aa-sequences -o GhostKOALA/WATER-protein-sequences.fa
anvi-get-sequences-for-gene-calls -c 03_CONTIGS/MAT-contigs.db --get-aa-sequences -o GhostKOALA/MAT-protein-sequences.fa
# ----------------Reformat fata headers -------------- #
#
sed -i 's/>/>genecall_/' 07_GhostKOALA/WATER-protein-sequences.fa
sed -i 's/>/>genecall_/' 07_GhostKOALA/MAT-protein-sequences.fa
# ----------------STOP!!! Run GhostKOALA-------------- #
# At this point you must upload modified gene calls to the SERVER
# http://www.kegg.jp/ghostkoala/
#
echo = `date` job $JOB_NAME don
</code></pre>
</details>
<h4 id="import-ghostkoala">Import GhostKOALA</h4>
<p>Once the jobs have finished we can import the data into our contig databases. In the repository you cloned earlier there is a file called <code>KO_Orthology_ko00001.txt</code>. <a href="http://merenlab.org/2018/01/17/importing-ghostkoala-annotations/#generate-the-kegg-orthology-table">See this</a> section for an explaination of the next step about converting the KEGG Orthology assignments to functions. We need this htext file to match the orthologies with function.</p>
<pre class="bash"><code>
mkdir GhostKOALA/GhostKoalaParser
wget &#39;https://www.genome.jp/kegg-bin/download_htext?htext=ko00001&amp;format=htext&amp;filedir=&#39; -O GhostKOALA/GhostKoalaParser/ko00001.keg
# Set the variable path
kegfile=&quot;GhostKOALA/GhostKoalaParser/ko00001.keg&quot;</code></pre>
<p>Now we need to parse the file using this code snippet from the tutorial.</p>
<pre class="bash"><code>
while read -r prefix content
do
    case &quot;$prefix&quot; in A) col1=&quot;$content&quot;;; \
                      B) col2=&quot;$content&quot; ;; \
                      C) col3=&quot;$content&quot;;; \
                      D) echo -e &quot;$col1\t$col2\t$col3\t$content&quot;;;
    esac
done &lt; &lt;(sed &#39;/^[#!+]/d;s/&lt;[^&gt;]*&gt;//g;s/^./&amp; /&#39; &lt; &quot;$kegfile&quot;) &gt; GhostKOALA/GhostKoalaParser/KO_Orthology_ko00001.txt</code></pre>
<p>Time to parse the annotation file…</p>
<pre class="bash"><code>
python GhostKOALA/GhostKoalaParser/KEGG-to-anvio --KeggDB GhostKOALA/GhostKoalaParser//KO_Orthology_ko00001.txt -i GhostKOALA/water_ko.txt -o GhostKOALA/water-KeggAnnotations-AnviImportable.txt
python GhostKOALA/GhostKoalaParser/KEGG-to-anvio --KeggDB GhostKOALA/GhostKoalaParser//KO_Orthology_ko00001.txt -i GhostKOALA/mat_ko.txt -o GhostKOALA/mat-KeggAnnotations-AnviImportable.txt</code></pre>
<p>…and parse the KEGG taxonomy</p>
<pre class="bash"><code>
python GhostKOALA/GhostKoalaParser/GhostKOALA-taxonomy-to-anvio GhostKOALA/water.out.top GhostKOALA/water-KeggTaxonomy.txt
python GhostKOALA/GhostKoalaParser/GhostKOALA-taxonomy-to-anvio GhostKOALA/mat.out.top GhostKOALA/mat-KeggTaxonomy.txt</code></pre>
<p>Now we can import it all into the anvio contig databases.</p>
<pre class="bash"><code>
anvi-import-functions -c 03_CONTIGS/WATER-contigs.db -i GhostKOALA/water-KeggAnnotations-AnviImportable.txt
anvi-import-functions -c 03_CONTIGS/MAT-contigs.db -i GhostKOALA/mat-KeggAnnotations-AnviImportable.txt
#
anvi-import-taxonomy-for-genes -c 03_CONTIGS/WATER-contigs.db -i GhostKOALA/water-KeggTaxonomy.txt -p default_matrix
anvi-import-taxonomy-for-genes -c 03_CONTIGS/MAT-contigs.db -i GhostKOALA/mat-KeggTaxonomy.txt -p default_matrix</code></pre>
<p>And that’s it!</p>
<details>
<summary>Show/hide HYDRA GhostKOALA parsing job script</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 20
#$ -q sThM.q
#$ -l mres=140G,h_data=7G,h_vmem=7G,himem
#$ -cwd
#$ -j y
#$ -N job_06_run_ghostkoala2
#$ -o hydra_logs/job_01_run_ghostkoala2.log
#$ -M scottjj@si.edu
#
# ----------------Modules------------------------- #
module load gcc/4.9.2
#
# ----------------Your Commands------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
# ----------------Activate Anvio -------------- #
#
export PATH=/home/scottjj/miniconda3:$PATH
export PATH=/home/scottjj/miniconda3/bin:$PATH
source activate anvio-master
#
which python
python --version
source /home/scottjj/virtual-envs/anvio-master/bin/activate
which python
python --version
which anvi-interactive
diamond --version
anvi-self-test -v
#
# ----------------Generate the KEGG orthology table-------------- #
mkdir GhostKOALA/GhostKoalaParser
wget 'https://www.genome.jp/kegg-bin/download_htext?htext=ko00001&format=htext&filedir=' -O GhostKOALA/GhostKoalaParser/ko00001.keg
#
kegfile="GhostKOALA/GhostKoalaParser/ko00001.keg"
#
while read -r prefix content
do
    case "$prefix" in A) col1="$content";; \
                      B) col2="$content" ;; \
                      C) col3="$content";; \
                      D) echo -e "$col1\t$col2\t$col3\t$content";;
    esac
done < <(sed '/^[#!+]/d;s/<[^>]*>//g;s/^./& /' < "$kegfile") > GhostKOALA/GhostKoalaParser/KO_Orthology_ko00001.txt
#
# ----------------Parsing the results from GhostKOALA-------------- #
#
python GhostKOALA/GhostKoalaParser/KEGG-to-anvio --KeggDB GhostKOALA/GhostKoalaParser//KO_Orthology_ko00001.txt -i GhostKOALA/water_ko.txt -o GhostKOALA/water-KeggAnnotations-AnviImportable.txt
python GhostKOALA/GhostKoalaParser/KEGG-to-anvio --KeggDB GhostKOALA/GhostKoalaParser//KO_Orthology_ko00001.txt -i GhostKOALA/mat_ko.txt -o GhostKOALA/mat-KeggAnnotations-AnviImportable.txt
#
python GhostKOALA/GhostKoalaParser/GhostKOALA-taxonomy-to-anvio GhostKOALA/water.out.top GhostKOALA/water-KeggTaxonomy.txt
python GhostKOALA/GhostKoalaParser/GhostKOALA-taxonomy-to-anvio GhostKOALA/mat.out.top GhostKOALA/mat-KeggTaxonomy.txt
# ----------------Importing GhostKOALA results-------------- #
#
anvi-import-functions -c 03_CONTIGS/WATER-contigs.db -i GhostKOALA/water-KeggAnnotations-AnviImportable.txt
anvi-import-functions -c 03_CONTIGS/MAT-contigs.db -i GhostKOALA/mat-KeggAnnotations-AnviImportable.txt
#
anvi-import-taxonomy-for-genes -c 03_CONTIGS/WATER-contigs.db -i GhostKOALA/water-KeggTaxonomy.txt -p default_matrix
anvi-import-taxonomy-for-genes -c 03_CONTIGS/MAT-contigs.db -i GhostKOALA/mat-KeggTaxonomy.txt -p default_matrix
#
echo = `date` job $JOB_NAME don
</code></pre>
</details>
<h2 id="merge-profiles-import-virsorter">Merge Profiles &amp; Import VirSorter</h2>
<p>Now that everything is added into the <code>contig.db</code> for the EP and WA assemblies and layer taxonomy is added to the individual <code>PROFILE.db</code>, its now time to merge all <code>PROFILE.db</code> into a single database for each assembly.</p>
<pre class="bash"><code>
anvi-merge 05_ANVIO_PROFILE/WATER/*/PROFILE.db -c 03_CONTIGS/WATER-contigs.db -o 06_MERGED/WATER</code></pre>
<details>
<summary>Show/hide HYDRA Merge Profile databases job details</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 2
#$ -q mThM.q
#$ -l mres=20G,h_data=10G,h_vmem=10G,himem
#$ -cwd
#$ -j y
#$ -N job_08_merge_profile_wa
#$ -o hydra_logs/job_08_merge_profile_wa.log
#
# ----------------Modules------------------------- #
module load gcc/4.9.2
#
# ----------------Your Commands------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
# ----------------Activate Anvio -------------- #
#
export PATH=/home/scottjj/miniconda3:$PATH
export PATH=/home/scottjj/miniconda3/bin:$PATH
source activate anvio-master
#
which python
python --version
source /home/scottjj/virtual-envs/anvio-master/bin/activate
which python
python --version
which anvi-interactive
diamond --version
anvi-self-test -v
#
# ----------------run anvio commands------------------- #
#
anvi-merge 05_ANVIO_PROFILE/WATER/*/PROFILE.db -c 03_CONTIGS/WATER-contigs.db -o 06_MERGED/WATER/
#
echo = `date` job $JOB_NAME done
</code></pre>
</details>
<h4 id="virsorter-import">Virsorter Import</h4>
<p>Now that we have a merged profile database for each assembly, we can deal with the virsorter data. We did this last for two reasons. First, we wanted to get a better idea of the total abundance of the viral community by looking at the other taxonomic annotations. Since the viral community is &gt; 10% we decided to use the virome decontamination data from Virsorter. Second, we need to add the VirSorter annotations to the merged <code>PROFILE.dbs</code> as a <code>COLLECTION</code>.</p>
<p>This is a multi-step process that is explained in great detail <a href="http://merenlab.org/2018/02/08/importing-virsorter-annotations/">here</a>. But please note that depending on the state of affairs with VirSorter and/or anvi’o, you may need to modify the output of <code>anvi-export-gene-calls</code> per VirSorter GitHub <a href="https://github.com/simroux/VirSorter/issues/65">issue 65</a>.</p>
<details>
<summary>Show/hide The issue and the fix</summary>
<pre><code>
The tool virsorter_to_anvio.py uses the output of anvi-export-gene-calls (among other files)
to import virsorter annotation into anvio dbs. The script relies on the output columns to be
in a particular order. That order has changed at some point in anvio's recent past.
#
VirSorter requires the following format:
gene_callers_id contig start stop direction partial source version
#
But it is now:
gene_callers_id contig direction partial source start stop version aa_sequence
#
So the columns need to be rearranged or the script changed.
I wish I could offer I script fix :) but I just used awk.
#
awk 'BEGIN {FS="\t"; OFS="\t"} {print $1, $2, $6, $7, $3, $4, $5, $8}' all_gene_calls_TEMP.txt > all_gene_calls.txt
#
This rearranges and eliminates the aa_sequence column.
</code></pre>
</details>
<p>First we need to grab some parsing scripts.</p>
<pre class="bash"><code>
wget https://raw.githubusercontent.com/brymerr921/VirSorterParser/master/virsorter_to_anvio.py -P $VIRSORTER/helper_scripts
wget https://raw.githubusercontent.com/brymerr921/VirSorterParser/master/hallmark_to_function_files/db1_hallmark_functions.txt -P $VIRSORTER/helper_scripts
wget https://raw.githubusercontent.com/brymerr921/VirSorterParser/master/hallmark_to_function_files/db2_hallmark_functions.txt -P $VIRSORTER/helper_scripts</code></pre>
<p>Next, export the files we need for the annotations from the contig databases.</p>
<pre class="bash"><code>
anvi-export-table 03_CONTIGS/WATER-contigs.db  --table splits_basic_info -o $VIRSORTER/WATER-splits_basic_info.txt
#
anvi-export-gene-calls -c 03_CONTIGS/WATER-contigs.db -o $VIRSORTER/WATER-all_gene_calls.txt</code></pre>
<p>Run the VirSorter parsing scripts.</p>
<pre class="bash"><code>
python $VIRSORTER/helper_scripts/virsorter_to_anvio.py --db 2 -a $VIRSORTER/WATER/Metric_files/VIRSorter_affi-contigs.tab -g $VIRSORTER/WATER/VIRSorter_global-phage-signal.csv -s $VIRSORTER/WATER-splits_basic_info.txt -n $VIRSORTER/WATER-all_gene_calls.txt -f $VIRSORTER/helper_scripts/db2_hallmark_functions.txt -A $VIRSORTER/WATER-virsorter_additional_info.txt -F $VIRSORTER/WATER-virsorter_annotations.txt -C $VIRSORTER/WATER-virsorter_collection.txt
#</code></pre>
<p>If you get an error you may need to reformat the gene calls files like so.</p>
<pre class="bash"><code>
awk &#39;BEGIN {FS=&quot;\t&quot;; OFS=&quot;\t&quot;} {print $1, $2, $6, $7, $3, $4, $5, $8}&#39; all_gene_calls_TEMP.txt &gt; all_gene_calls.txt</code></pre>
<p>Finally, import all of the data in to the contig and profile dbs from each assembly.</p>
<pre class="bash"><code>
anvi-import-misc-data $VIRSORTER/WATER-virsorter_additional_info.txt -p 06_MERGED/WATER/PROFILE.db  --target-data-table items
anvi-import-collection $VIRSORTER/WATER-virsorter_collection.txt -c 03_CONTIGS/WATER-contigs.db -p 06_MERGED/WATER/PROFILE.db -C VIRSORTER
anvi-import-functions -c 03_CONTIGS/WATER-contigs.db -i $VIRSORTER/WATER-virsorter_annotations.txt</code></pre>
<details>
<summary>Show/hide HYDRA Virsorter annotation import job details</summary>
<pre><code>
# /bin/sh
# ----------------Parameters---------------------- #
#$ -S /bin/sh
#$ -pe mthread 10
#$ -q sThC.q
#$ -l mres=50G,h_data=5G,h_vmem=5G
#$ -cwd
#$ -j y
#$ -N job_09_parse_virsorter
#$ -o hydra_logs/job_09_parse_virsorter.log
#
# ----------------Modules------------------------- #
module load gcc/4.9.2
#
# ----------------Your Commands------------------- #
#
echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS
#
# ----------------Activate Anvio -------------- #
#
export PATH=/home/scottjj/miniconda3:$PATH
export PATH=/home/scottjj/miniconda3/bin:$PATH
source activate anvio-master
#
which python
python --version
source /home/scottjj/virtual-envs/anvio-master/bin/activate
which python
python --version
which anvi-interactive
diamond --version
anvi-self-test -v
#
# ----------------Get VirSorter Scripts------------------- #
#
VIRSORTER='/pool/genomics/stri_istmobiome/data/HYPOXIA_DATA/HYPOXIA/07_TAXONOMY/VIRSORTER_VIROME'
#
wget https://raw.githubusercontent.com/brymerr921/VirSorterParser/master/virsorter_to_anvio.py -P $VIRSORTER/helper_scripts
wget https://raw.githubusercontent.com/brymerr921/VirSorterParser/master/hallmark_to_function_files/db1_hallmark_functions.txt -P $VIRSORTER/helper_scripts
wget https://raw.githubusercontent.com/brymerr921/VirSorterParser/master/hallmark_to_function_files/db2_hallmark_functions.txt -P $VIRSORTER/helper_scripts
#
# ----------------Export files for annotation------------------- #
#
anvi-export-table 03_CONTIGS/WATER-contigs.db  --table splits_basic_info -o $VIRSORTER/WATER-splits_basic_info.txt
anvi-export-gene-calls -c 03_CONTIGS/WATER-contigs.db -o $VIRSORTER/WATER-all_gene_calls_TEMP.txt --gene-caller prodigal
#
# ----------------REORDER columns !!!!!!!!MAY NOT NEED TO DO THIS!!!!!------------------- #
#
awk 'BEGIN {FS="\t"; OFS="\t"} {print $1, $2, $6, $7, $3, $4, $5, $8}' $VIRSORTER/WATER-all_gene_calls_TEMP.txt > $VIRSORTER/WATER-all_gene_calls.txt
awk 'BEGIN {FS="\t"; OFS="\t"} {print $1, $2, $6, $7, $3, $4, $5, $8}' $VIRSORTER/MAT-all_gene_calls_TEMP.txt > $VIRSORTER/MAT-all_gene_calls.txt
#
# ----------------run virsorter command for anvio import------------------- #
#
python $VIRSORTER/helper_scripts/virsorter_to_anvio.py --db 2 -a $VIRSORTER/WATER/Metric_files/VIRSorter_affi-contigs.tab -g $VIRSORTER/WATER/VIRSorter_global-phage-signal.csv -s $VIRSORTER/WATER-splits_basic_info.txt -n $VIRSORTER/WATER-all_gene_calls.txt -f $VIRSORTER/helper_scripts/db2_hallmark_functions.txt -A $VIRSORTER/WATER-virsorter_additional_info.txt -F $VIRSORTER/WATER-virsorter_annotations.txt -C $VIRSORTER/WATER-virsorter_collection.txt
#
# ----------------import annotation data with anvio------------------- #
#
anvi-import-misc-data $VIRSORTER/WATER-virsorter_additional_info.txt -p 06_MERGED/WATER/PROFILE.db  --target-data-table items
anvi-import-collection $VIRSORTER/WATER-virsorter_collection.txt -c 03_CONTIGS/WATER-contigs.db -p 06_MERGED/WATER/PROFILE.db -C VIRSORTER
anvi-import-functions -c 03_CONTIGS/WATER-contigs.db -i $VIRSORTER/WATER-virsorter_annotations.txt
#
deactivate
#
echo = `date` job $JOB_NAME done
</code></pre>
</details>
<h2 id="conclusion">Conclusion</h2>
<p>This section of the workflow is complete. Lets take a look at the what we have so far.</p>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 80%" />
</colgroup>
<thead>
<tr class="header">
<th>Directory</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>00_LOGS/</code></td>
<td>Individual log files for each step of the snakemake workflow.</td>
</tr>
<tr class="even">
<td><code>00_TRIMMED/</code></td>
<td>Eight trimmed, compressed fastq files for each sample.</td>
</tr>
<tr class="odd">
<td><code>01_QC/</code></td>
<td>Merged forward (R1) &amp; reverse (R2) QC’ed fastq files. QC STATS file for each sample. <code>qc-report.txt</code> file, a summary table of all QC results.</td>
</tr>
<tr class="even">
<td><code>02_FASTA/</code></td>
<td>A contig fasta file and reformat report for each assembly.</td>
</tr>
<tr class="odd">
<td><code>03_CONTIGS/</code></td>
<td>An annotated contig database for each assembly.</td>
</tr>
<tr class="even">
<td><code>04_MAPPING/</code></td>
<td>Short read BAM files.</td>
</tr>
<tr class="odd">
<td><code>05_ANVIO_PROFILE/</code></td>
<td>Individual profile database for each sample.</td>
</tr>
<tr class="even">
<td><code>06_MERGED/</code></td>
<td>Single merged profile database.</td>
</tr>
</tbody>
</table>
<p><br/></p>
<div class="post-nav">
<div class="post-nav-item">
<div class="meta-nav">
Previous
</div>
<p><a href="mg-databases.html" rel="next">N<sup><u>o</u></sup> 2. Annotation Databases</a></p>
</div>
</div>
<div class="post-nav">
<div class="post-nav-item">
<div class="meta-nav">
Next
</div>
<p><a href="mg-workflow-2.html" rel="prev">N<sup><u>o</u></sup> 4. Data Summary</a></p>
</div>
</div>
<h2 id="source-code" class="appendix">Source Code</h2>
<p>The source code for this page can be accessed on GitHub by <a href="https://github.com/hypocolypse/web/blob/master/mg-workflow-1.Rmd">clicking this link</a>.</p>
<div id="refs" class="references">
<div id="ref-bolger2014trimmomatic">
<p>Bolger, Anthony M, Marc Lohse, and Bjoern Usadel. 2014. “Trimmomatic: A Flexible Trimmer for Illumina Sequence Data.” <em>Bioinformatics</em> 30 (15): 2114–20. <a href="https://doi.org/10.1093/bioinformatics/btu170">https://doi.org/10.1093/bioinformatics/btu170</a>.</p>
</div>
<div id="ref-breitwieser2018krakenuniq">
<p>Breitwieser, FP, DN Baker, and Steven L Salzberg. 2018. “KrakenUniq: Confident and Fast Metagenomics Classification Using Unique K-Mer Counts.” <em>Genome Biology</em> 19 (1): 1–10. <a href="https://doi.org/10.5281/zenodo.1412647">https://doi.org/10.5281/zenodo.1412647</a>.</p>
</div>
<div id="ref-buchfink2015fast">
<p>Buchfink, Benjamin, Chao Xie, and Daniel H Huson. 2015. “Fast and Sensitive Protein Alignment Using Diamond.” <em>Nature Methods</em> 12 (1): 59–60. <a href="https://doi.org/10.1038/nmeth.3176">https://doi.org/10.1038/nmeth.3176</a>.</p>
</div>
<div id="ref-delmont2018nitrogen">
<p>Delmont, Tom O, Christopher Quince, Alon Shaiber, Özcan C Esen, Sonny TM Lee, Michael S Rappé, Sandra L McLellan, Sebastian Lücker, and A Murat Eren. 2018. “Nitrogen-Fixing Populations of Planctomycetes and Proteobacteria Are Abundant in Surface Ocean Metagenomes.” <em>Nature Microbiology</em> 3 (7): 804–13. <a href="https://doi.org/10.1038/s41564-018-0176-9">https://doi.org/10.1038/s41564-018-0176-9</a>.</p>
</div>
<div id="ref-eren2015anvi">
<p>Eren, A Murat, Özcan C Esen, Christopher Quince, Joseph H Vineis, Hilary G Morrison, Mitchell L Sogin, and Tom O Delmont. 2015. “Anvi’o: An Advanced Analysis and Visualization Platform for ‘Omics Data.” <em>PeerJ</em> 3: e1319. <a href="https://doi.org/10.7717/peerj.1319">https://doi.org/10.7717/peerj.1319</a>.</p>
</div>
<div id="ref-eren2013filtering">
<p>Eren, A Murat, Joseph H Vineis, Hilary G Morrison, and Mitchell L Sogin. 2013. “A Filtering Method to Generate High Quality Short Reads Using Illumina Paired-End Technology.” <em>PLoS One</em> 8 (6). <a href="https://doi.org/10.1371/journal.pone.0066643">https://doi.org/10.1371/journal.pone.0066643</a>.</p>
</div>
<div id="ref-hyatt2010prodigal">
<p>Hyatt, Doug, Gwo-Liang Chen, Philip F LoCascio, Miriam L Land, Frank W Larimer, and Loren J Hauser. 2010. “Prodigal: Prokaryotic Gene Recognition and Translation Initiation Site Identification.” <em>BMC Bioinformatics</em> 11 (1): 119. <a href="https://doi.org/10.1186/1471-2105-11-119">https://doi.org/10.1186/1471-2105-11-119</a>.</p>
</div>
<div id="ref-kim2016centrifuge">
<p>Kim, Daehwan, Li Song, Florian P Breitwieser, and Steven L Salzberg. 2016. “Centrifuge: Rapid and Sensitive Classification of Metagenomic Sequences.” <em>Genome Research</em> 26 (12): 1721–9. <a href="https://doi.org/10.1101/gr.210641.116.">https://doi.org/10.1101/gr.210641.116.</a></p>
</div>
<div id="ref-koster2012snakemake">
<p>Köster, Johannes, and Sven Rahmann. 2012. “Snakemake—a Scalable Bioinformatics Workflow Engine.” <em>Bioinformatics</em> 28 (19): 2520–2. <a href="https://doi.org/10.1093/bioinformatics/bts480">https://doi.org/10.1093/bioinformatics/bts480</a>.</p>
</div>
<div id="ref-langmead2012fast">
<p>Langmead, Ben, and Steven L Salzberg. 2012. “Fast Gapped-Read Alignment with Bowtie 2.” <em>Nature Methods</em> 9 (4): 357. <a href="https://doi.org/10.1038/nmeth.1923">https://doi.org/10.1038/nmeth.1923</a>.</p>
</div>
<div id="ref-li2015megahit">
<p>Li, Dinghua, Chi-Man Liu, Ruibang Luo, Kunihiko Sadakane, and Tak-Wah Lam. 2015. “MEGAHIT: An Ultra-Fast Single-Node Solution for Large and Complex Metagenomics Assembly via Succinct de Bruijn Graph.” <em>Bioinformatics</em> 31 (10): 1674–6. <a href="https://doi.org/10.1093/bioinformatics/btv033">https://doi.org/10.1093/bioinformatics/btv033</a>.</p>
</div>
<div id="ref-li2009sequence">
<p>Li, Heng, Bob Handsaker, Alec Wysoker, Tim Fennell, Jue Ruan, Nils Homer, Gabor Marth, Goncalo Abecasis, and Richard Durbin. 2009. “The Sequence Alignment/Map Format and Samtools.” <em>Bioinformatics</em> 25 (16): 2078–9. <a href="https://doi.org/10.1093/bioinformatics/btp352">https://doi.org/10.1093/bioinformatics/btp352</a>.</p>
</div>
<div id="ref-menzel2016fast">
<p>Menzel, Peter, Kim Lee Ng, and Anders Krogh. 2016. “Fast and Sensitive Taxonomic Classification for Metagenomics with Kaiju.” <em>Nature Communications</em> 7: 11257. <a href="https://doi.org/10.1038/ncomms11257">https://doi.org/10.1038/ncomms11257</a>.</p>
</div>
<div id="ref-ondov2011krona">
<p>Ondov, Brian D, Nicholas H Bergman, and Adam M Phillippy. 2011. “Interactive Metagenomic Visualization in a Web Browser.” <em>BMC Bioinformatics</em> 12 (1): 385. <a href="https://doi.org/10.1186/1471-2105-12-385">https://doi.org/10.1186/1471-2105-12-385</a>.</p>
</div>
<div id="ref-roux2015virsorter">
<p>Roux, Simon, Francois Enault, Bonnie L Hurwitz, and Matthew B Sullivan. 2015. “VirSorter: Mining Viral Signal from Microbial Genomic Data.” <em>PeerJ</em> 3: e985. <a href="https://doi.org/10.1093/bioinformatics/btu170">https://doi.org/10.1093/bioinformatics/btu170</a>.</p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="updates-and-corrections">Corrections</h3>
<p>If you see mistakes or want to suggest changes, please <a href="https://github.com/hypocolypse/web/issues/new">create an issue</a> on the source repository.</p>
<h3 id="reuse">Reuse</h3>
<p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. Source code is available at <a href="https://github.com/hypocolypse/web/">https://github.com/hypocolypse/web/</a>, unless otherwise noted. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
</div>
<script id="distill-bibliography" type="text/bibtex">
@article{callahan2016dada2,
  title={DADA2: high-resolution sample inference from Illumina amplicon data},
  author={Callahan, Benjamin J and McMurdie, Paul J and Rosen, Michael J and Han, Andrew W and Johnson, Amy Jo A and Holmes, Susan P},
  journal={Nature Methods},
  volume={13},
  number={7},
  pages={581},
  year={2016},
  url = {https://doi.org/10.1038/nmeth.3869},
}

@article{martin2011cutadapt,
  title={Cutadapt removes adapter sequences from high-throughput sequencing reads},
  author={Martin, Marcel},
  journal={EMBnet. journal},
  volume={17},
  number={1},
  pages={10--12},
  year={2011},
  url = {https://doi.org/10.14806/ej.17.1.200}
}

@article{quast2012silva,
  title={The SILVA ribosomal RNA gene database project: improved data processing and web-based tools},
  author={Quast, Christian and Pruesse, Elmar and Yilmaz, Pelin and Gerken, Jan and Schweer, Timmy and Yarza, Pablo and Peplies, J{\"o}rg and Gl{\"o}ckner, Frank Oliver},
  journal={Nucleic Acids Research},
  volume={41},
  number={D1},
  pages={D590--D596},
  year={2012},
  url = {https://doi.org/10.1093/nar/gks1219}
}

@article{mcmurdie2013phyloseq,
  title={phyloseq: an R package for reproducible interactive analysis and graphics of microbiome census data},
  author={McMurdie, Paul J and Holmes, Susan},
  journal={PLoS One},
  volume={8},
  number={4},
  pages={e61217},
  year={2013},
  url = {https://doi.org/10.1371/journal.pone.0061217}
}

@misc{wong2011points,
  title={Points of view: Color blindness},
  author={Wong, Bang},
  journal={Nature Methods},
  volume={8},
  number={6},
  pages={441},
  year={2011},
  url = {https://doi.org/10.1038/nmeth.1618}
}

@misc{lahti2017microbiome,
  author = {Lahti, Leo and Shetty Sudarshan and others},
  title = {Tools for microbiome analysis in R. Version 1.9.97},
  year = {2017},
  url = {https://github.com/microbiome/microbiome},
}

@article{eren2015anvi,
  title={Anvi’o: an advanced analysis and visualization platform for ‘omics data},
  author={Eren, A Murat and Esen, {\"O}zcan C and Quince, Christopher and Vineis, Joseph H and Morrison, Hilary G and Sogin, Mitchell L and Delmont, Tom O},
  journal={PeerJ},
  volume={3},
  pages={e1319},
  year={2015},
  url = {https://doi.org/10.7717/peerj.1319}
}
@article{bolger2014trimmomatic,
  title={Trimmomatic: a flexible trimmer for Illumina sequence data},
  author={Bolger, Anthony M and Lohse, Marc and Usadel, Bjoern},
  journal={Bioinformatics},
  volume={30},
  number={15},
  pages={2114--2120},
  year={2014},
  url = {https://doi.org/10.1093/bioinformatics/btu170}
}
@article{koster2012snakemake,
  title={Snakemake—a scalable bioinformatics workflow engine},
  author={K{\"o}ster, Johannes and Rahmann, Sven},
  journal={Bioinformatics},
  volume={28},
  number={19},
  pages={2520--2522},
  year={2012},
  url = {https://doi.org/10.1093/bioinformatics/bts480}
}
@article{menzel2016fast,
  title={Fast and sensitive taxonomic classification for metagenomics with Kaiju},
  author={Menzel, Peter and Ng, Kim Lee and Krogh, Anders},
  journal={Nature Communications},
  volume={7},
  pages={11257},
  year={2016},
  url = {https://doi.org/10.1038/ncomms11257}
}
@article{roux2015virsorter,
  title={VirSorter: mining viral signal from microbial genomic data},
  author={Roux, Simon and Enault, Francois and Hurwitz, Bonnie L and Sullivan, Matthew B},
  journal={PeerJ},
  volume={3},
  pages={e985},
  year={2015},
  url = {https://doi.org/10.1093/bioinformatics/btu170}
}
@article{kim2016centrifuge,
  title={Centrifuge: rapid and sensitive classification of metagenomic sequences},
  author={Kim, Daehwan and Song, Li and Breitwieser, Florian P and Salzberg, Steven L},
  journal={Genome Research},
  volume={26},
  number={12},
  pages={1721--1729},
  year={2016},
  url = {https://doi.org/10.1101/gr.210641.116.}
}
@article{hyatt2010prodigal,
  title={Prodigal: prokaryotic gene recognition and translation initiation site identification},
  author={Hyatt, Doug and Chen, Gwo-Liang and LoCascio, Philip F and Land, Miriam L and Larimer, Frank W and Hauser, Loren J},
  journal={BMC Bioinformatics},
  volume={11},
  number={1},
  pages={119},
  year={2010},
  url = {https://doi.org/10.1186/1471-2105-11-119}
}
@article{edgar2004muscle,
  title={MUSCLE: multiple sequence alignment with high accuracy and high throughput},
  author={Edgar, Robert C},
  journal={Nucleic Acids Research},
  volume={32},
  number={5},
  pages={1792--1797},
  year={2004},
  url = {https://doi.org/10.1093/nar/gkh340}
}
@article{li2015megahit,
  title={MEGAHIT: an ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph},
  author={Li, Dinghua and Liu, Chi-Man and Luo, Ruibang and Sadakane, Kunihiko and Lam, Tak-Wah},
  journal={Bioinformatics},
  volume={31},
  number={10},
  pages={1674--1676},
  year={2015},
  url = {https://doi.org/10.1093/bioinformatics/btv033}
}
@article{langmead2012fast,
  title={Fast gapped-read alignment with Bowtie 2},
  author={Langmead, Ben and Salzberg, Steven L},
  journal={Nature Methods},
  volume={9},
  number={4},
  pages={357},
  year={2012},
  url = {https://doi.org/10.1038/nmeth.1923}
}
@article{li2009fast,
  title={Fast and accurate short read alignment with Burrows--Wheeler transform},
  author={Li, Heng and Durbin, Richard},
  journal={Bioinformatics},
  volume={25},
  number={14},
  pages={1754--1760},
  year={2009},
  url = {https://doi.org/10.1093/bioinformatics/btp324}
}
@article{li2009sequence,
  title={The sequence alignment/map format and SAMtools},
  author={Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
  journal={Bioinformatics},
  volume={25},
  number={16},
  pages={2078--2079},
  year={2009},
  url = {https://doi.org/10.1093/bioinformatics/btp352}
}

@article{breitwieser2018krakenuniq,
  title={KrakenUniq: confident and fast metagenomics classification using unique k-mer counts},
  author={Breitwieser, FP and Baker, DN and Salzberg, Steven L},
  journal={Genome Biology},
  volume={19},
  number={1},
  pages={1--10},
  year={2018},
  url = {https://doi.org/10.5281/zenodo.1412647}
}

@article{eren2013filtering,
  title={A filtering method to generate high quality short reads using Illumina paired-end technology},
  author={Eren, A Murat and Vineis, Joseph H and Morrison, Hilary G and Sogin, Mitchell L},
  journal={PLoS One},
  volume={8},
  number={6},
  year={2013},
  url = {https://doi.org/10.1371/journal.pone.0066643}
}

@article{alneberg2014binning,
  title={Binning metagenomic contigs by coverage and composition},
  author={Alneberg, Johannes and Bjarnason, Brynjar Sm{\'a}ri and De Bruijn, Ino and Schirmer, Melanie and Quick, Joshua and Ijaz, Umer Z and Lahti, Leo and Loman, Nicholas J and Andersson, Anders F and Quince, Christopher},
  journal={Nature Methods},
  volume={11},
  number={11},
  pages={1144--1146},
  year={2014},
  url = {https://doi.org/10.1038/nmeth.3103}
}
@article{sieber2018recovery,
  title={Recovery of genomes from metagenomes via a dereplication, aggregation and scoring strategy},
  author={Sieber, Christian MK and Probst, Alexander J and Sharrar, Allison and Thomas, Brian C and Hess, Matthias and Tringe, Susannah G and Banfield, Jillian F},
  journal={Nature Microbiology},
  volume={3},
  number={7},
  pages={836--843},
  year={2018},
}

@article{kang2019metabat,
  title={MetaBAT 2: an adaptive binning algorithm for robust and efficient genome reconstruction from metagenome assemblies},
  author={Kang, Dongwan D and Li, Feng and Kirton, Edward and Thomas, Ashleigh and Egan, Rob and An, Hong and Wang, Zhong},
  journal={PeerJ},
  volume={7},
  pages={e7359},
  year={2019},
}
@article{wu2016maxbin,
  title={MaxBin 2.0: an automated binning algorithm to recover genomes from multiple metagenomic datasets},
  author={Wu, Yu-Wei and Simmons, Blake A and Singer, Steven W},
  journal={Bioinformatics},
  volume={32},
  number={4},
  pages={605--607},
  year={2016},
}

@article{ondov2011krona,
  title={Interactive metagenomic visualization in a Web browser},
  author={Ondov, Brian D and Bergman, Nicholas H and Phillippy, Adam M},
  journal={BMC Bioinformatics},
  volume={12},
  number={1},
  pages={385},
  year={2011},
  url = {https://doi.org/10.1186/1471-2105-12-385}
}

@article{buchfink2015fast,
  title={Fast and sensitive protein alignment using DIAMOND},
  author={Buchfink, Benjamin and Xie, Chao and Huson, Daniel H},
  journal={Nature Methods},
  volume={12},
  number={1},
  pages={59--60},
  year={2015},
  url = {https://doi.org/10.1038/nmeth.3176}
}
@article{delmont2018nitrogen,
  title={Nitrogen-fixing populations of Planctomycetes and Proteobacteria are abundant in surface ocean metagenomes},
  author={Delmont, Tom O and Quince, Christopher and Shaiber, Alon and Esen, {\"O}zcan C and Lee, Sonny TM and Rapp{\'e}, Michael S and McLellan, Sandra L and L{\"u}cker, Sebastian and Eren, A Murat},
  journal={Nature Microbiology},
  volume={3},
  number={7},
  pages={804--813},
  year={2018},
  url = {https://doi.org/10.1038/s41564-018-0176-9}
}
</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<div class="distill-site-nav distill-site-footer">
<p>© Copyright 2020 The HYPOCOLYPSE Project.</p>
<p>Site constructed using <a href="https://rstudio.github.io/distill/">Distill for R Markdown</a>.</p>
</div>
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
